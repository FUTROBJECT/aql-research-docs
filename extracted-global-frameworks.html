<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Extracted Framework Profiles - Global AI Governance Research</title>
<style>
  body {
    font-family: Arial, sans-serif;
    font-size: 11pt;
    line-height: 1.5;
    max-width: 8.5in;
    margin: 0 auto;
    padding: 1in;
    color: #333;
  }
  h1 {
    font-size: 18pt;
    border-bottom: 2px solid #333;
    padding-bottom: 8px;
    margin-bottom: 4px;
  }
  h2 {
    font-size: 14pt;
    border-bottom: 1px solid #ccc;
    padding-bottom: 6px;
    margin-top: 24px;
  }
  h3 {
    font-size: 12pt;
    border-bottom: 1px solid #eee;
    padding-bottom: 4px;
    margin-top: 16px;
  }
  table {
    border-collapse: collapse;
    width: 100%;
    font-size: 10pt;
    margin: 12px 0;
  }
  th, td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: left;
    vertical-align: top;
  }
  th {
    background-color: #f0f0f0;
    font-weight: bold;
  }
  .metadata {
    color: #666;
    font-size: 10pt;
  }
  .subtitle {
    font-size: 14pt;
    color: #666;
    margin-top: -8px;
  }
  .callout {
    background: #e7f3ff;
    border-left: 4px solid #0066cc;
    padding: 12px 16px;
    margin: 12px 0;
    font-size: 10pt;
  }
  .critical {
    background: #f8d7da;
    border-left: 4px solid #c00;
    padding: 12px 16px;
    margin: 12px 0;
    font-size: 10pt;
  }
  .success {
    background: #d4edda;
    border-left: 4px solid #28a745;
    padding: 12px 16px;
    margin: 12px 0;
    font-size: 10pt;
  }
  .highlight-box {
    background: #fff3cd;
    border: 1px solid #ffc107;
    padding: 12px 16px;
    margin: 12px 0;
    font-size: 10pt;
  }
  .row-critical {
    background-color: #e7f3ff;
  }
  .row-high {
    background-color: #d4edda;
  }
  .row-medium-high {
    background-color: #fff3cd;
  }
  .relevance-label-critical {
    color: #0066cc;
    font-weight: bold;
  }
  .relevance-label-high {
    color: #28a745;
    font-weight: bold;
  }
  .relevance-label-medium-high {
    color: #856404;
    font-weight: bold;
  }
  .about-box {
    background: #f5f5f5;
    border: 1px solid #ddd;
    padding: 16px;
    font-size: 10pt;
    font-style: italic;
    margin: 24px 0;
  }
  .framework-card {
    background: #fff;
    border: 1px solid #dee2e6;
    padding: 20px;
    margin: 24px 0;
    border-radius: 4px;
  }
  .framework-header {
    background: #0066cc;
    color: white;
    padding: 12px 16px;
    margin: -20px -20px 16px -20px;
    font-size: 13pt;
    font-weight: bold;
    border-radius: 4px 4px 0 0;
  }
  .framework-meta {
    display: flex;
    gap: 16px;
    flex-wrap: wrap;
    margin: 12px 0;
    font-size: 10pt;
    color: #666;
  }
  .meta-tag {
    background: #f0f0f0;
    padding: 2px 8px;
    border-radius: 3px;
    font-size: 9pt;
  }
  .relevance-high {
    border-left: 4px solid #28a745;
  }
  .relevance-medium {
    border-left: 4px solid #ffc107;
  }
  .relevance-critical {
    border-left: 4px solid #0066cc;
  }
  .toc {
    background: #f8f9fa;
    padding: 20px;
    margin: 20px 0;
    border: 1px solid #ddd;
  }
  .toc a {
    color: #0066cc;
    text-decoration: none;
  }
  .toc a:hover {
    text-decoration: underline;
  }
  .toc ul {
    list-style: none;
    padding-left: 0;
  }
  .toc ul li {
    padding: 3px 0;
  }
  .principle-box {
    background: #f8f9fa;
    border: 1px solid #dee2e6;
    border-left: 4px solid #0066cc;
    padding: 12px 16px;
    margin: 8px 0;
    font-size: 10pt;
  }
  .principle-box strong {
    color: #0066cc;
  }
  .area-box {
    background: #f8f9fa;
    border: 1px solid #dee2e6;
    border-left: 4px solid #17a2b8;
    padding: 12px 16px;
    margin: 8px 0;
    font-size: 10pt;
  }
  .area-box strong {
    color: #17a2b8;
  }
  ul.relevant-elements {
    font-size: 10pt;
  }
  .education-highlight {
    background: #fff3cd;
    font-weight: bold;
    padding: 2px 4px;
  }
  @media print {
    body { padding: 0.5in; }
    .framework-card { page-break-inside: avoid; }
  }
</style>
</head>
<body>

<div class="logo-container" style="display: flex; align-items: flex-end; margin-bottom: 20px;">
  <svg id="FO_Logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 172.97 142.95" style="width: 80px; height: auto;">
    <defs>
      <style>
        .cls-1 {
          font-family: Switzer-SemiboldItalic, Switzer, Arial, sans-serif;
          font-size: 130.25px;
          font-style: italic;
          font-weight: 600;
        }
        .cls-2 {
          letter-spacing: -.02em;
        }
        .cls-3 {
          letter-spacing: -.04em;
        }
      </style>
    </defs>
    <text class="cls-1" transform="translate(0 112.21)"><tspan class="cls-3" x="0" y="0">F</tspan><tspan class="cls-2" x="70.47" y="0">O</tspan></text>
  </svg>
  <span style="font-family: Arial, sans-serif; font-size: 12px; font-weight: 600; letter-spacing: 0.08em; color: #333; margin-left: 4px; padding-bottom: 12px;">RESEARCH</span>
</div>

<h1>Extracted Framework Profiles</h1>
<p class="subtitle">21 AI Governance Frameworks Analyzed for Community College Relevance</p>

<p class="metadata">
<strong>Date:</strong> February 2026<br>
<strong>Phase:</strong> Framework Extraction (Phase 2)<br>
<strong>Prepared by:</strong> AQL Labs / FutureObjects<br>
<strong>Commissioned by:</strong> College Futures Foundation
</p>

<!-- FRAMEWORK OVERVIEW TABLE -->
<h2>Framework Overview</h2>
<p>Summary of 21 governance frameworks analyzed for community college relevance, sorted by applicability rating.</p>

<table>
  <tr>
    <th>Framework</th>
    <th>Organization</th>
    <th>Year</th>
    <th>Type</th>
    <th>Scope</th>
    <th>CC Applicability</th>
  </tr>

  <!-- Critical -->
  <tr class="row-critical">
    <td>Creating the AI-Enabled Community College</td>
    <td>Achieving the Dream (ATD)</td>
    <td>2025</td>
    <td>Guidance</td>
    <td>Sector</td>
    <td class="relevance-label-critical">Critical</td>
  </tr>
  <tr class="row-critical">
    <td>EDUCAUSE AI Governance Resources</td>
    <td>EDUCAUSE</td>
    <td>2024&ndash;2025</td>
    <td>Guidance</td>
    <td>Sector</td>
    <td class="relevance-label-critical">Critical</td>
  </tr>
  <tr class="row-critical">
    <td>SACSCOC/C-RAC AI Guidance for Accreditation</td>
    <td>SACSCOC / C-RAC</td>
    <td>2024&ndash;2025</td>
    <td>Guidance</td>
    <td>Sector</td>
    <td class="relevance-label-critical">Critical</td>
  </tr>
  <tr class="row-critical">
    <td>AI Handbook for Local Government</td>
    <td>U. Michigan STPP / MI Municipal League</td>
    <td>2024</td>
    <td>Toolkit</td>
    <td>National</td>
    <td class="relevance-label-critical">Critical</td>
  </tr>
  <tr class="row-critical">
    <td>Singapore Agentic AI Framework</td>
    <td>IMDA / CSA / GovTech Singapore</td>
    <td>2025&ndash;2026</td>
    <td>Guidance</td>
    <td>National</td>
    <td class="relevance-label-critical">Critical</td>
  </tr>

  <!-- High -->
  <tr class="row-high">
    <td>WCET AI Education Policy &amp; Practice Ecosystem Framework</td>
    <td>WCET / WICHE</td>
    <td>2023&ndash;2025</td>
    <td>Guidance</td>
    <td>Sector</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>Coalition for Health AI (CHAI) Responsible Health AI Framework</td>
    <td>CHAI</td>
    <td>2024&ndash;2025</td>
    <td>Standard</td>
    <td>Sector</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>GIDA: CARE Principles &amp; Indigenous Peoples&rsquo; Rights in Data</td>
    <td>Global Indigenous Data Alliance</td>
    <td>2019&ndash;2025</td>
    <td>Guidance</td>
    <td>International</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>Data Cooperatives Governance Model</td>
    <td>Aapti Institute / Data2X / Open Data Manchester</td>
    <td>2024</td>
    <td>Toolkit</td>
    <td>International</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>ASCCC Academic Integrity Policies in the Age of AI</td>
    <td>ASCCC</td>
    <td>2024</td>
    <td>Guidance</td>
    <td>Sector</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>IEEE P7004 - Child and Student Data Governance</td>
    <td>IEEE Standards Association</td>
    <td>2020&ndash;In Dev.</td>
    <td>Standard</td>
    <td>International</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>OMB M-25-21 - Federal AI Governance</td>
    <td>White House OMB</td>
    <td>2025</td>
    <td>Policy</td>
    <td>National</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>GAO AI Accountability Framework</td>
    <td>U.S. Government Accountability Office</td>
    <td>2021</td>
    <td>Guidance</td>
    <td>National</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>NIST AI 600-1 - GenAI Risk Profile</td>
    <td>NIST</td>
    <td>2024</td>
    <td>Standard</td>
    <td>National</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>NAIC Model Bulletin - AI in Insurance</td>
    <td>NAIC</td>
    <td>2023&ndash;2025</td>
    <td>Guidance</td>
    <td>Sector</td>
    <td class="relevance-label-high">High</td>
  </tr>
  <tr class="row-high">
    <td>The Ethical Framework for AI in Education</td>
    <td>Institute for Ethical AI in Education</td>
    <td>2021</td>
    <td>Framework</td>
    <td>International</td>
    <td class="relevance-label-high">High</td>
  </tr>

  <!-- Medium-High -->
  <tr class="row-medium-high">
    <td>NACUBO AI Governance &amp; Cooperative Procurement Ecosystem</td>
    <td>NACUBO / E&amp;I Cooperative Services</td>
    <td>2024&ndash;2025</td>
    <td>Platform</td>
    <td>Sector</td>
    <td class="relevance-label-medium-high">Medium-High</td>
  </tr>
  <tr class="row-medium-high">
    <td>Advance CTE Career Clusters - AI Integration</td>
    <td>Advance CTE / ACTE</td>
    <td>2024&ndash;2025</td>
    <td>Guidance</td>
    <td>Sector</td>
    <td class="relevance-label-medium-high">Medium-High</td>
  </tr>
  <tr class="row-medium-high">
    <td>UNESCO Indigenous Data Sovereignty in AI</td>
    <td>UNESCO</td>
    <td>2024&ndash;2025</td>
    <td>Guidance</td>
    <td>International</td>
    <td class="relevance-label-medium-high">Medium-High</td>
  </tr>

  <!-- Medium -->
  <tr>
    <td>Freedom Online Coalition - AI and Human Rights</td>
    <td>Freedom Online Coalition (42 nations)</td>
    <td>2024&ndash;2025</td>
    <td>Policy</td>
    <td>International</td>
    <td>Medium</td>
  </tr>
  <tr>
    <td>HEAT-AI: Higher Education Act for AI</td>
    <td>St. P&ouml;lten University of Applied Sciences</td>
    <td>2024&ndash;2025</td>
    <td>Guidance</td>
    <td>Institutional</td>
    <td>Medium</td>
  </tr>
</table>

<p><strong>Critical</strong> (blue) &mdash; Directly addresses CC contexts or provides essential governance structure.<br>
<strong>High</strong> (green) &mdash; Transferable governance models with strong CC application.<br>
<strong>Medium-High</strong> (yellow) &mdash; Addresses specific CC-relevant domains (CTE, data sovereignty).<br>
<strong>Medium</strong> &mdash; Indirect relevance; informs broader policy context.</p>

<hr>

<!-- TABLE OF CONTENTS -->
<div class="toc">
  <h2 style="margin-top: 0; border-bottom: none;">Table of Contents</h2>

  <p><strong>CC-Specific Frameworks:</strong></p>
  <ul>
    <li>1. <a href="#asccc">ASCCC Academic Integrity Policies in the Age of AI</a></li>
    <li>2. <a href="#atd">ATD Creating the AI-Enabled Community College</a></li>
  </ul>

  <p><strong>Education &amp; Accreditation:</strong></p>
  <ul>
    <li>3. <a href="#ieee-7004">IEEE P7004 - Child and Student Data Governance</a></li>
    <li>4. <a href="#educause">EDUCAUSE AI Governance Resources</a></li>
    <li>5. <a href="#sacscoc">SACSCOC/C-RAC AI Guidance for Accreditation</a></li>
    <li>6. <a href="#advance-cte">Advance CTE Career Clusters - AI Integration</a></li>
  </ul>

  <p><strong>Federal &amp; National Policy:</strong></p>
  <ul>
    <li>7. <a href="#omb">OMB M-25-21 - Federal AI Governance</a></li>
    <li>8. <a href="#gao">GAO AI Accountability Framework</a></li>
    <li>9. <a href="#nist">NIST AI 600-1 - GenAI Risk Profile</a></li>
  </ul>

  <p><strong>Cross-Sector Models:</strong></p>
  <ul>
    <li>10. <a href="#naic">NAIC Model Bulletin - AI in Insurance</a></li>
    <li>11. <a href="#local-gov">AI Handbook for Local Government</a></li>
    <li>12. <a href="#singapore">Singapore Agentic AI Framework</a></li>
  </ul>

  <p><strong>International &amp; Human Rights:</strong></p>
  <ul>
    <li>13. <a href="#unesco-indigenous">UNESCO Indigenous Data Sovereignty in AI</a></li>
    <li>14. <a href="#foc">Freedom Online Coalition - AI and Human Rights</a></li>
  </ul>

  <p><strong>Education-Specific (Cross-listed):</strong></p>
  <ul>
    <li>15. <a href="#ieaied">The Ethical Framework for AI in Education</a></li>
  </ul>

  <p><strong>Network Governance Models:</strong></p>
  <ul>
    <li>16. <a href="#chai">Coalition for Health AI (CHAI) Responsible Health AI Framework</a></li>
    <li>17. <a href="#wcet">WCET AI Education Policy &amp; Practice Ecosystem Framework</a></li>
    <li>18. <a href="#nacubo">NACUBO AI Governance &amp; Cooperative Procurement Ecosystem</a></li>
    <li>19. <a href="#gida">GIDA: CARE Principles &amp; Indigenous Peoples&rsquo; Rights in Data</a></li>
    <li>20. <a href="#data-coops">Data Cooperatives Governance Model</a></li>
    <li>21. <a href="#heat-ai">HEAT-AI: Higher Education Act for AI</a></li>
  </ul>
</div>

<hr>

<!-- ============================================================ -->
<!-- FRAMEWORK 1: ASCCC -->
<!-- ============================================================ -->
<div class="framework-card" id="asccc">
  <div class="framework-header">1. ASCCC Academic Integrity Policies in the Age of Artificial Intelligence (AI Resources 2024)</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Academic Senate for California Community Colleges (ASCCC)</span>
    <span><strong>Year:</strong> 2024</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> California / United States</span>
    <span><strong>Document Link</strong><a href="https://www.asccc.org/sites/default/files/ASCCC_AI_Resources_2024.pdf"> Download PDF</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide a framework and guiding principles for faculty and local academic senates to use within collegial consultation processes when revising academic and professional policies related to AI and academic integrity. Developed in response to Resolution 13.05 SP23 adopted at Spring 2023 Plenary Session.</p>
  <p><strong>Key Principles:</strong> Five core principles for AI policies: (1) Ethical considerations, (2) Legal compliance, (3) Transparency and wide communication, (4) Accountability and oversight, (5) Support through professional learning, training, and education.</p>
  <p><strong>Target Audience:</strong> Faculty, local academic senates, curriculum committees, and college administrators in California's 116 community colleges serving 1.8+ million students.</p>
  <p><strong>Enforcement:</strong> Voluntary guidance for local adoption through collegial consultation; grounded in Title 5 '10+1' faculty governance authority over academic and professional matters.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Faculty-led through local academic senates using collegial consultation with administration. ASCCC provides statewide guidance; local senates develop campus-specific policies. AI policy committees should include diverse faculty representation appointed by Academic Senate.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Faculty primacy emphasized. Collaboration with California Community Colleges Chancellor's Office (CCCCO) and Faculty Association of California Community Colleges (FACCC). Students informed through clear syllabi language and communication.</td></tr>
    <tr><td>Accountability</td><td>Local academic senate oversight. Regular evaluations of AI policies, systems' effectiveness, fairness, and impact recommended. AI and GenAI Technologies Caucus provides ongoing advice to ASCCC and local senates.</td></tr>
    <tr><td>Remedy Process</td><td>Through local governance structures. Emphasis on clear communication of expectations and processes for addressing concerns.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Faculty maintain authority over curriculum and academic standards under Title 5 '10+1'. AI positioned as tool to support&mdash;not replace&mdash;faculty judgment. Faculty role in developing, implementing, and evaluating AI policies emphasized.</td></tr>
    <tr><td>Transparency</td><td>Policies must be 'widely communicated and transparent.' Syllabi should include clear AI use expectations. Institutions should require transparency in AI systems deployment, making algorithm and data information accessible to relevant stakeholders.</td></tr>
    <tr><td>Non-Discrimination</td><td>Equity considerations central. Notes AI can increase accessibility (text-to-speech, captioning, keyboard navigation) but warns of paywalls limiting access. Includes suggested reading for algorithmic justice. Addresses impact on different student demographics including those with learning disabilities and ESL learners.</td></tr>
    <tr><td>Data Governance</td><td>AI Governance defined as 'policies and regulations governing development, deployment, and use of AI technologies in higher education, ensuring ethical and responsible AI practices, data security, and compliance with legal requirements.'</td></tr>
    <tr><td>Risk Assessment</td><td>AI Evaluation Tool developed for faculty to assess AI tools across dimensions of ethical use, pedagogical impact, usability, and sustainability. Research questions recommended for evaluating long-term effects on learning outcomes.</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First comprehensive AI governance framework specifically designed for community colleges by a statewide faculty governance body. Grounds AI governance in existing Title 5 faculty authority over academic and professional matters ('10+1'). Provides sample syllabi language, glossary of AI terms, and algorithmic justice readings. Recognizes varying levels of AI familiarity among educators. Treats framework as 'ongoing work in progress due to transformational nature of AI.'
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Directly designed for California Community Colleges
  </div>
  <ul class="relevant-elements">
    <li>Five-principle framework adaptable to any CC context</li>
    <li>Sample syllabi language for academic integrity policies</li>
    <li>AI Evaluation Tool methodology for assessing educational AI</li>
    <li>Governance structure grounded in faculty shared governance</li>
    <li>Equity-centered approach addressing accessibility and cost barriers</li>
    <li>Recognition of faculty professional development needs</li>
    <li>Integration with existing collegial consultation processes</li>
    <li>Glossary providing common vocabulary for AI discussions</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework is California-specific (Title 5, '10+1' governance) but principles transfer to other states. May need adaptation for: different state governance structures, specific accreditation requirements, institutional size variations. Non-credit and workforce education applications not explicitly addressed.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 2: ATD -->
<!-- ============================================================ -->
<div class="framework-card" id="atd">
  <div class="framework-header">2. Creating the AI-Enabled Community College: A Road Map for Using Generative AI to Accelerate Student Success</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Achieving the Dream (ATD) AI for All Task Force</span>
    <span><strong>Year:</strong> 2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States (300+ community college network)</span>
    <span><strong>Document Link</strong><a href="https://achievingthedream.org/wp-content/uploads/2025/07/ATD-AI-Task-Force-Report-Final-7-21-25.pdf"> Download Link(PDF)</a></span>
  </div> 

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide a strategic framework for community colleges to integrate generative AI across teaching, operations, and workforce partnerships in ways that center students, uphold community college values, and accelerate student success. Addresses both opportunities and risks of AI for community colleges serving diverse student populations.</p>
  <p><strong>Key Principles:</strong> Core concept of 'AI Agility' &mdash; moving beyond basic AI literacy to the ability to continually adapt alongside rapidly evolving technologies. Framework centered on equity, ethics, student success, and workforce alignment. Emphasizes AI as 'force multiplier' for overburdened institutions while warning against unchecked systems that could deepen inequities.</p>
  <p><strong>Target Audience:</strong> Community college presidents, leadership teams, faculty, staff, and boards. Designed for ATD's 300+ member network but applicable to all community colleges nationally.</p>
  <p><strong>Enforcement:</strong> Voluntary framework. ATD will integrate AI agility into coaching and support services and develop professional learning opportunities around the eight action areas.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Recommends cross-functional teams empowered to leverage AI in alignment with institutional priorities. Strategic leadership from presidents and boards with distributed implementation across college functions.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Task force composed of community college presidents, AI experts, and industry partners. Framework involves students, faculty, and leadership in ethics discussions. Emphasizes faculty engagement as critical to success.</td></tr>
    <tr><td>Accountability</td><td>Action Area 3 focuses on developing assessments to measure AI user impact. Calls for continuous human oversight and regular bias audits. Data-informed culture with measurement of outcomes.</td></tr>
    <tr><td>Remedy Process</td><td>Warns that 'unchecked AI systems can perpetuate biases, undermine equity, and erode trust.' Recommends mechanisms for identifying and addressing unintended consequences.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Continuous human oversight emphasized throughout. Faculty positioned as essential to AI integration success. Students and faculty involved in ethics discussions. AI augments rather than replaces human judgment.</td></tr>
    <tr><td>Transparency</td><td>Recommends 'ethics across the curriculum' approach situating AI in context of academic integrity and ethical behaviors. Calls for clear communication of AI policies and expectations.</td></tr>
    <tr><td>Non-Discrimination</td><td>Equity is central concern. Warns AI 'could deepen existing inequities' without thoughtful integration. Calls for regular bias audits. Notes AI can 'extend reach of overburdened institutions' and 'support economic mobility' when used ethically.</td></tr>
    <tr><td>Data Governance</td><td>Integrated within ethical governance framework. Addresses data use in AI-supported student advising and engagement. Privacy considerations embedded in student success applications.</td></tr>
    <tr><td>Risk Assessment</td><td>Action Area 2 establishes ethical AI governance structures. Framework acknowledges 'unprecedented opportunities and serious risks.' Addresses concerns about academic integrity and workforce preparation.</td></tr>
  </table>

  <h3>Eight Action Areas</h3>
  <ol>
    <li><strong>Demonstrate Strategic Leadership</strong> &mdash; Presidential and board leadership in AI strategy development. Creating data-informed, technology-forward culture. Integrating AI into broader strategic goals.</li>
    <li><strong>Establish Ethical AI Governance</strong> &mdash; Developing governance structures and policies for responsible AI use. Ethics across the curriculum approach. Regular bias audits and continuous human oversight.</li>
    <li><strong>Develop Assessments to Measure AI User Impact</strong> &mdash; Creating metrics and assessment approaches to understand AI's effects on students, faculty, and institutional outcomes.</li>
    <li><strong>Build Staff Capabilities</strong> &mdash; Developing AI skills and knowledge among all college staff to support institution-wide adoption.</li>
    <li><strong>Support Professional Learning and Faculty Engagement</strong> &mdash; Faculty development for AI integration in teaching. Overcoming barriers to ensure faculty are AI-agile. Supporting faculty to help students evolve their AI skills.</li>
    <li><strong>Redesign the Curriculum</strong> &mdash; Integrating AI across curriculum. Establishing AI literacy as core competency. Elevating role of humanities with stronger learning outcomes in ethics, logic, and critical analysis.</li>
    <li><strong>Leverage AI for Workforce Alignment</strong> &mdash; Preparing students for AI-transformed workplaces. Addressing that 26% of working adults use AI on the job. Connecting curriculum to employer AI skill needs.</li>
    <li><strong>Invest in Student Success Through AI</strong> &mdash; Using AI to personalize learning, support advising, and improve student outcomes. AI as force multiplier for student success initiatives.</li>
  </ol>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First comprehensive AI framework developed specifically by and for community colleges through a major national network. Introduces 'AI Agility' concept distinguishing from basic literacy. Eight action areas provide structured implementation pathway. Bridges academic programs and workforce development. Addresses both degree-seeking and non-degree pathway students. Emphasizes role of humanities and critical thinking alongside technical AI skills. Provides network-wide implementation support through ATD coaching services.
  </div>

  <h3>CC Relevance</h3>
  <div class="callout relevance-critical" style="padding: 12px 16px;">
    <strong>Applicability: Critical</strong> &mdash; Designed specifically for community colleges
  </div>
  <ul class="relevant-elements">
    <li>Eight action areas provide comprehensive implementation roadmap</li>
    <li>AI Agility concept applicable to student learning outcomes</li>
    <li>Equity-centered approach aligns with CC mission</li>
    <li>Addresses both degree and non-degree pathways</li>
    <li>Workforce alignment guidance for CTE programs</li>
    <li>Faculty engagement model for shared governance contexts</li>
    <li>Ethics across curriculum approach</li>
    <li>Professional development framework for resource-constrained institutions</li>
    <li>Network-based implementation support model</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework is directly applicable to any US community college. May need local adaptation for: specific governance structures, accreditation requirements, state regulatory contexts, institutional size and resources. Strong alignment with California CC system priorities.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 3: IEEE P7004 -->
<!-- ============================================================ -->
<div class="framework-card" id="ieee-7004">
  <div class="framework-header">3. IEEE P7004 - Standard for Child and Student Data Governance</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> IEEE Standards Association / IEEE Computer Society</span>
    <span><strong>Year:</strong> 2020 (PAR approved), In development</span>
    <span class="meta-tag">Standard</span>
    <span class="meta-tag">International</span>
    <span><strong>Geographic Focus:</strong> Global</span>
    <span><strong>Document Link</strong><a href="https://standards.ieee.org/ieee/7004/10270/"> Web Source</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Define specific methodologies to help users certify how they approach accessing, collecting, storing, utilizing, sharing, and destroying child and student data. Provide processes and certifications for transparency and accountability for educational institutions handling data to ensure student safety.</p>
  <p><strong>Key Principles:</strong> Certifiable and responsible child and student data governance methodologies. Specific metrics and conformance criteria from trusted global sources. Transparency and accountability throughout data lifecycle. Protection of minors in academic environments.</p>
  <p><strong>Target Audience:</strong> Educational institutions, EdTech vendors, educational technology developers, school administrators, privacy officers, and any organization handling child or student data.</p>
  <p><strong>Enforcement:</strong> Certifiable standard with specific metrics and conformance criteria. Part of IEEE certification ecosystem.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>IEEE Standards Association governance. Working Group WG-CSDG (Child and Student Data Governance) under IEEE Computer Society Learning Technology Committee (C/LT). Working Group Chair: Boris Radanovic.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Open working group accepting participants. International participation including representatives from European standards bodies (CEN/CENELEC, ISO) and national bodies. StandICT-funded fellows from multiple countries.</td></tr>
    <tr><td>Accountability</td><td>Certifiable standard with defined metrics. Conformance criteria enable third-party assessment and certification.</td></tr>
    <tr><td>Remedy Process</td><td>Standard development process allows for revisions. Related to broader IEEE 7000 series ethical framework.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Part of IEEE 7000 series which emphasizes human values in autonomous and intelligent systems design. Data governance requires human decision-making in handling sensitive student information.</td></tr>
    <tr><td>Transparency</td><td>Core focus on transparency in data handling. Requires clear processes for how data is accessed, collected, stored, utilized, shared, and destroyed.</td></tr>
    <tr><td>Non-Discrimination</td><td>Related to IEEE 7003 (Algorithmic Bias) which addresses bias mitigation in AI systems. Student data governance supports equitable treatment.</td></tr>
    <tr><td>Data Governance</td><td>PRIMARY FOCUS &mdash; Defines complete data lifecycle governance: (1) Accessing child/student data, (2) Collecting data, (3) Storing data, (4) Utilizing data, (5) Sharing data, (6) Destroying data. Applies to any educational or institutional setting.</td></tr>
    <tr><td>Risk Assessment</td><td>Certification approach enables risk-based assessment of data handling practices. Metrics and conformance criteria provide measurable standards.</td></tr>
  </table>

  <h3>Data Lifecycle Coverage</h3>
  <table>
    <tr><th>Phase</th><th>Description</th></tr>
    <tr><td>Accessing</td><td>Defines methods for appropriate access to child and student data</td></tr>
    <tr><td>Collecting</td><td>Standards for responsible data collection practices</td></tr>
    <tr><td>Storing</td><td>Requirements for secure data storage</td></tr>
    <tr><td>Utilizing</td><td>Guidelines for appropriate use of student data</td></tr>
    <tr><td>Sharing</td><td>Protocols for sharing data with relevant parties</td></tr>
    <tr><td>Destroying</td><td>Procedures for appropriate data removal and deletion</td></tr>
  </table>

  <h3>Related Standards (IEEE 7000 Series)</h3>
  <table>
    <tr><th>Standard</th><th>Title</th><th>Relationship</th></tr>
    <tr><td>IEEE 7000 (2021)</td><td>Model Process for Addressing Ethical Concerns During System Design</td><td>Parent standard providing overarching ethical design methodology</td></tr>
    <tr><td>IEEE 7001</td><td>Transparency of Autonomous Systems</td><td>Complementary standard on system transparency</td></tr>
    <tr><td>IEEE 7002</td><td>Data Privacy Process</td><td>Related standard on embedding data privacy in development; shares common ground on personal data management</td></tr>
    <tr><td>IEEE 7003</td><td>Algorithmic Bias Considerations</td><td>Complementary standard on bias mitigation in AI systems</td></tr>
    <tr><td>IEEE 7004.1</td><td>Recommended Practices for Virtual Classroom Security, Privacy and Data Governance</td><td>Extension providing best practices for meeting P7004 requirements in online/virtual classroom contexts</td></tr>
    <tr><td>IEEE 7005</td><td>Employer Data Governance</td><td>Parallel governance structure for workforce/employer data context</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Only international standard specifically focused on child and student data governance. Provides certifiable methodologies rather than just principles. Part of comprehensive IEEE 7000 ethical AI series but specialized for education context. Addresses unique vulnerabilities and protections required for minors in academic environments. Applicable to any educational setting &mdash; K-12, higher education, vocational training.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Directly addresses student data governance in educational settings
  </div>
  <ul class="relevant-elements">
    <li>Certifiable data governance methodology for educational institutions</li>
    <li>Complete data lifecycle coverage (access, collect, store, use, share, destroy)</li>
    <li>Metrics and conformance criteria for assessment</li>
    <li>Applicable to higher education including community colleges</li>
    <li>IEEE 7004.1 extension addresses online learning environments</li>
    <li>RFP language for procurement of educational technology</li>
    <li>Certification framework for vendor evaluation</li>
    <li>International applicability for diverse student populations</li>
  </ul>
  <p><em>Adaptation Needed:</em> Standard is in development &mdash; final requirements pending. Community colleges should: (1) Monitor standard development and participate in working group if possible, (2) Use current framework to inform data governance policies, (3) Reference IEEE 7004.1 for online learning data governance, (4) Align with FERPA and state privacy requirements, (5) Consider certification once standard is finalized.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 4: EDUCAUSE -->
<!-- ============================================================ -->
<div class="framework-card" id="educause">
  <div class="framework-header">4. EDUCAUSE AI Governance Resources: Action Plan, Landscape Studies, and Ethical Guidelines</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> EDUCAUSE</span>
    <span><strong>Year:</strong> 2024-2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States / International Higher Education</span>
    <span><strong>Document Link</strong><a href="https://library.educause.edu/resources/2024/2/2024-educause-ai-landscape-study"> Web Source</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide comprehensive framework to help ensure institutional AI-related policies and guidelines address critical aspects of institutional life. Document higher education community's sentiments and experiences with AI. Establish ethical frameworks for responsible AI implementation while upholding academic values.</p>
  <p><strong>Key Principles:</strong> Fairness, privacy, transparency, and accountability. Shared governance aligning institutional intent with individual responsibility. Equity and accuracy in AI deployment. Mitigation of bias, privacy violations, and misuse risks.</p>
  <p><strong>Target Audience:</strong> Higher education technology professionals, CIOs, institutional leadership, faculty, staff, and administrators across all institution types.</p>
  <p><strong>Enforcement:</strong> Voluntary guidance and benchmarking. Annual landscape studies provide institutional comparison data.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Four-level policy development framework: Individual, Department/Unit, Institution, Multi-institution. AI governing body recommended for institutional oversight.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Engage students and faculty on AI use and ethics. Cross-departmental collaboration to break down silos. Multi-institution consultation with universities and private sector.</td></tr>
    <tr><td>Accountability</td><td>Governance frameworks with defined teams, executive sponsorship, charter, and shared signals. Annual landscape studies track institutional progress.</td></tr>
    <tr><td>Remedy Process</td><td>Risk assessments and monitoring. Policies for ethical and effective decision-making about AI use.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Governance frameworks align institutional intent with individual responsibility. Faculty and staff monitoring of AI usage.</td></tr>
    <tr><td>Transparency</td><td>Public trust through transparency efforts. Clear policies and guidelines for ethical decision-making.</td></tr>
    <tr><td>Non-Discrimination</td><td>Emphasis on inclusive and equitable access. Guidelines fostering equity. Mitigation of bias.</td></tr>
    <tr><td>Data Governance</td><td>Data governance explicitly included in policy coverage areas. Risk assessments recommended.</td></tr>
    <tr><td>Risk Assessment</td><td>Risk assessments as mitigation strategy. Evaluation of AI use across institution.</td></tr>
  </table>

  <h3>Four-Level Policy Framework</h3>
  <div class="area-box">
    <strong>Level 1 &mdash; Individual:</strong> Engage students and faculty. Find out how they use generative AI and how they feel about ethics and impact on learning.
  </div>
  <div class="area-box">
    <strong>Level 2 &mdash; Department/Unit:</strong> Academic programs. Assess role of generative AI in academic programs, find common ground between departments, break down silos.
  </div>
  <div class="area-box">
    <strong>Level 3 &mdash; Institution:</strong> Institutional governance. Establish AI governing body for oversight and guidelines that foster equity and accuracy.
  </div>
  <div class="area-box">
    <strong>Level 4 &mdash; Multi-Institution:</strong> External collaboration. Consult with other universities and private sector organizations on generative AI challenges.
  </div>

  <h3>Key Findings</h3>
  <h4>2024 Landscape Study</h4>
  <ul>
    <li>Only <strong>23%</strong> of respondents indicated institution has AI-related acceptable use policies in place</li>
    <li>Nearly half (<strong>48%</strong>) disagreed or strongly disagreed that institution has appropriate policies for ethical and effective AI decision-making</li>
  </ul>

  <h4>2025 Landscape Study</h4>
  <ul>
    <li>More than half of institutions working on AI-related strategy in some areas, but no institution-wide approach. Only 11% report no AI-related strategy.</li>
    <li>Marked increase in enthusiasm for AI and slight decrease in apprehensive attitudes (2024 to 2025).</li>
    <li>Institutions primarily accommodating AI work by upskilling existing staff. 61% have AI-related work responsibilities (up from 56% in 2024).</li>
    <li><strong>Digital AI Divide:</strong> Successful AI adoption demands staggering array of resources &mdash; funding, staffing, leadership, strategic/operational resilience, technology/data infrastructure &mdash; some institutions have these more readily than others.</li>
  </ul>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Primary higher education AI governance resource in the US. Annual landscape studies provide sector-wide benchmarking data. Four-level framework addresses governance from individual to multi-institutional. Identifies 'digital AI divide' affecting resource-constrained institutions. Tracks policy adoption progress across sector. Connects governance to academic values and teaching/learning mission.
  </div>

  <h3>CC Relevance</h3>
  <div class="callout relevance-critical" style="padding: 12px 16px;">
    <strong>Applicability: Critical</strong> &mdash; Primary higher education AI governance resource
  </div>
  <ul class="relevant-elements">
    <li>Four-level policy framework applicable to CC governance structures</li>
    <li>Landscape study data provides benchmarking for CC institutions</li>
    <li>Digital AI divide directly relevant to resource-constrained CCs</li>
    <li>Policy gap data (23% with policies) contextualizes CC challenges</li>
    <li>Faculty and staff upskilling emphasis matches CC needs</li>
    <li>Equity and access focus aligns with CC mission</li>
    <li>Multi-institution consultation model for system collaboration</li>
    <li>Coverage of promotion/tenure practices (adaptable for CC context)</li>
  </ul>
  <p><em>Adaptation Needed:</em> EDUCAUSE resources primarily developed for four-year institutions. Adaptations for CC: (1) Address part-time/adjunct faculty majority, (2) Workforce/CTE program considerations, (3) Open access mission implications, (4) Transfer student data considerations, (5) Non-credit program coverage. Resource constraints ('digital AI divide') particularly acute for CCs.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 5: SACSCOC/C-RAC -->
<!-- ============================================================ -->
<div class="framework-card" id="sacscoc">
  <div class="framework-header">5. SACSCOC/C-RAC AI Guidance for Accreditation</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> SACSCOC / Council of Regional Accrediting Commissions (C-RAC)</span>
    <span><strong>Year:</strong> 2024-2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States Higher Education</span>
    <span><strong>Document Link</strong><a href="https://sacscoc.org/app/uploads/2024/12/AI-in-Accreditation.pdf"> Web Source</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide guidance on AI use in accreditation processes and institutional operations. Clarify that responsible AI use does not conflict with accreditation standards. Support institutions in developing AI policies that maintain academic integrity while enabling innovation.</p>
  <p><strong>Key Principles:</strong> AI use in learning evaluation does not conflict with accreditation standards. Institutions maintain responsibility for quality assurance regardless of AI involvement. Transparency in AI use is essential. Human judgment remains central to accreditation decisions.</p>
  <p><strong>Target Audience:</strong> Higher education institutions, accreditation bodies, academic leadership, faculty, and quality assurance professionals.</p>
  <p><strong>Enforcement:</strong> Guidance rather than mandate. Institutions expected to demonstrate responsible AI use during accreditation reviews. Standards-based accountability through existing accreditation processes.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Institutional autonomy in AI policy development within accreditation standards framework. Accreditors provide guidance rather than prescriptive requirements.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Faculty involvement in policy development emphasized. Student awareness and training expected. Cross-functional institutional committees recommended.</td></tr>
    <tr><td>Accountability</td><td>AI use documented in accreditation self-studies and compliance reports. Peer review during accreditation visits assesses AI governance. Standards-based evaluation continues to apply.</td></tr>
    <tr><td>Remedy Process</td><td>Existing accreditation appeals and review processes apply. Institutions can seek clarification from accreditors on AI-related questions.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Human judgment central to accreditation decisions. AI supports but does not replace faculty and staff decision-making. Oversight required for AI-assisted evaluation.</td></tr>
    <tr><td>Transparency</td><td>Documentation of AI use in institutional processes. Clear communication to students about AI in assessment. Disclosure in accreditation materials.</td></tr>
    <tr><td>Non-Discrimination</td><td>Equity considerations in AI deployment. Attention to bias in AI-assisted evaluation. Access considerations for all student populations.</td></tr>
    <tr><td>Data Governance</td><td>Institutional responsibility for data used in AI systems. Privacy protection in AI applications. Validation of AI tools before deployment.</td></tr>
    <tr><td>Risk Assessment</td><td>Evaluation of AI tools before implementation. Ongoing monitoring of AI system performance. Quality assurance processes for AI-assisted decisions.</td></tr>
  </table>

  <h3>C-RAC Statement Principles</h3>
  <div class="principle-box">
    <strong>Statement on the Use of Artificial Intelligence (AI) to Advance Learning Evaluation and Recognition</strong> (October 6, 2025)
  </div>
  <p>Key affirmation: <em>"The use of AI in learning evaluation does not conflict with accreditation standards."</em></p>
  <ul>
    <li>AI can enhance efficiency and consistency in credential evaluation</li>
    <li>Human oversight remains essential for quality assurance</li>
    <li>Institutions bear responsibility for AI tool validation</li>
    <li>Transparency in AI-assisted evaluation processes is required</li>
    <li>Continuous improvement and monitoring of AI systems expected</li>
  </ul>
  <p style="font-size: 10pt; color: #666;">Scope: Applies across all C-RAC member commissions including SACSCOC, Middle States, HLC, NECHE, NWCCU, WASC Senior, and WSCUC.</p>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First coordinated regional accreditor guidance on AI in higher education. C-RAC statement provides unified position across all US regional accreditors. Affirms AI compatibility with accreditation standards, reducing institutional uncertainty. Bridges gap between innovation pressure and quality assurance. Establishes documentation and transparency expectations. Provides foundation for institution-level AI policy development.
  </div>

  <h3>CC Relevance</h3>
  <div class="callout relevance-critical" style="padding: 12px 16px;">
    <strong>Applicability: Critical</strong> &mdash; All accredited CCs must align with these expectations
  </div>
  <ul class="relevant-elements">
    <li>Affirmation that AI use does not conflict with accreditation standards</li>
    <li>Framework for developing institutional AI policies</li>
    <li>Documentation requirements for accreditation reviews</li>
    <li>Faculty training and development expectations</li>
    <li>Student communication requirements</li>
    <li>Quality assurance integration guidance</li>
    <li>Human oversight emphasis applicable to student services AI</li>
    <li>Transparency requirements for AI-assisted assessment</li>
  </ul>
  <p><em>Adaptation Needed:</em> Guidance is general; CCs need to operationalize for: (1) Specific AI tools in use (tutoring, advising, enrollment), (2) Part-time/adjunct faculty training logistics, (3) Open access student population considerations, (4) Workforce program assessment contexts, (5) Transfer credit evaluation with AI. SACSCOC is primary accreditor for many Southern CCs; C-RAC statement applies to all regionally-accredited CCs.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 6: ADVANCE CTE -->
<!-- ============================================================ -->
<div class="framework-card" id="advance-cte">
  <div class="framework-header">6. Advance CTE Career Clusters Framework - AI Integration and CTE AI Policy</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Advance CTE / Association for Career and Technical Education (ACTE)</span>
    <span><strong>Year:</strong> 2024-2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States</span>
    <span><strong>Document Link</strong><a href="https://careertech.org/wp-content/uploads/2024/06/Advancing_National_Career_Clusters-Framework_Draft_V3.2_June_2024.pdf"> Download Link</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Integrate artificial intelligence across modernized Career Clusters Framework. Provide policy guidance for AI in career technical education. Support CTE professionals in understanding AI's impact on workforce preparation and program delivery.</p>
  <p><strong>Key Principles:</strong> AI as cross-cutting skill relevant to all career pathways. Workforce alignment with emerging technology demands. Integration of AI into existing CTE program structures rather than standalone framework.</p>
  <p><strong>Target Audience:</strong> CTE administrators, educators, policy makers, state CTE directors, community college workforce programs, and industry partners.</p>
  <p><strong>Enforcement:</strong> Voluntary guidance. Federal reporting alignment with Perkins V required starting 2025-26 academic year.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>National framework with state flexibility. States may adopt modernized Framework in whole or part. Federal alignment through Perkins V reporting.</td></tr>
    <tr><td>Stakeholder Participation</td><td>CTE administrators, educators, industry partners. State CTE directors shaping implementation. Community college role emphasized for accessibility.</td></tr>
    <tr><td>Accountability</td><td>Federal reporting requirements through Perkins V Consolidated Annual Report (CAR). Data submission starting January 2027.</td></tr>
    <tr><td>Remedy Process</td><td>State-level policy processes. Perkins V compliance mechanisms.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>CTE programs emphasize human skills alongside technical AI skills. Faculty role in curriculum development and program delivery.</td></tr>
    <tr><td>Transparency</td><td>Career Clusters Framework publicly available. State flexibility in implementation allows local adaptation.</td></tr>
    <tr><td>Non-Discrimination</td><td>Perkins V includes equity provisions. NAPEQUITY monitors equity in CTE implementation.</td></tr>
    <tr><td>Data Governance</td><td>Federal reporting requirements provide accountability structure.</td></tr>
    <tr><td>Risk Assessment</td><td>Not explicitly addressed in current guidance; emerging area.</td></tr>
  </table>

  <h3>Career Clusters AI Integration</h3>
  <div class="area-box">
    <strong>Digital Technology Cluster:</strong> Focuses on developing digital systems for communication and data storage using critical technologies. Artificial intelligence (AI) explicitly listed as core technology alongside data analytics and cybersecurity. Skills necessary for all careers to navigate and lead in constantly evolving tech landscape.
  </div>
  <div class="area-box">
    <strong>Advanced Manufacturing Cluster:</strong> Blends innovative technologies and practices to enhance design and production. Automation and artificial intelligence explicitly included alongside engineering, R&D, equipment maintenance, safety protocols, and quality control.
  </div>
  <div class="area-box">
    <strong>Framework Modernization (2024):</strong> New technologies added include: Automation and robotics, Clean and alternative energy, Artificial intelligence and unmanned vehicles. States may adopt new Framework in whole or part, reflecting needs of states and local communities.
  </div>

  <h3>Perkins V Implications</h3>
  <table>
    <tr><th>Aspect</th><th>Details</th></tr>
    <tr><td>Federal Reporting</td><td>U.S. Department of Education requires states to align Perkins data with modernized Framework starting 2025-26 academic year</td></tr>
    <tr><td>CAR Submission</td><td>Data submission for federal Perkins Consolidated Annual Report (CAR) starting January 2027</td></tr>
    <tr><td>Funding</td><td>More than $1.3 billion appropriated in FY2021 for Perkins V</td></tr>
    <tr><td>Purpose</td><td>Develop academic knowledge and technical and employability skills of secondary and postsecondary students in CTE programs</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Note: As of January 2026, Advance CTE has NOT published a standalone comprehensive AI governance framework for CTE. AI is integrated into the modernized Career Clusters Framework and addressed through policy advocacy. This extraction captures existing AI-related guidance and resources rather than a single framework document. AI is treated as a cross-cutting skill relevant to all career pathways, with federal reporting alignment through Perkins V creating accountability structures.
  </div>

  <h3>CC Relevance</h3>
  <div class="highlight-box" style="padding: 12px 16px;">
    <strong>Applicability: Medium-High</strong> &mdash; Career Clusters Framework directly impacts CC CTE programs
  </div>
  <ul class="relevant-elements">
    <li>AI integration in Digital Technology and Advanced Manufacturing clusters</li>
    <li>Perkins V funding and reporting requirements</li>
    <li>Federal AI education executive orders</li>
    <li>Community college role in AI workforce development</li>
    <li>Cross-sector AI skills transferability</li>
    <li>State flexibility in framework implementation</li>
  </ul>
  <p><em>Adaptation Needed:</em> Community colleges need to develop local AI governance for CTE programs since no comprehensive national framework exists. Should: (1) Integrate AI ethics into existing CTE curriculum, (2) Develop faculty AI competencies, (3) Align with state Perkins plans, (4) Connect with industry partners for current AI applications, (5) Address equity in AI access for CTE students.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 7: OMB M-25-21 -->
<!-- ============================================================ -->
<div class="framework-card" id="omb">
  <div class="framework-header">7. OMB M-25-21: Accelerating Federal Use of AI through Innovation, Governance, and Public Trust</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> White House Office of Management and Budget (OMB)</span>
    <span><strong>Year:</strong> 2025</span>
    <span class="meta-tag">Policy</span>
    <span class="meta-tag">National</span>
    <span><strong>Geographic Focus:</strong> United States Federal Government</span>
    <span><strong>Document Link</strong><a href="https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-21-Accelerating-Federal-Use-of-AI-through-Innovation-Governance-and-Public-Trust.pdf"> Download Link</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Replace Biden-era M-24-10 with 'forward-leaning and pro-innovation' approach to accelerate federal AI adoption while maintaining safeguards. Support implementation of Executive Order 14179 ('Removing Barriers to American Leadership in Artificial Intelligence'). Focus on three key priorities: Innovation, Governance, and Public Trust.</p>
  <p><strong>Key Principles:</strong> Rapid, responsible AI adoption. Risk-based approach targeting high-impact AI. Streamlined requirements with agency leadership discretion. American-made AI prioritization. Balance innovation acceleration with civil rights/liberties protection.</p>
  <p><strong>Target Audience:</strong> Federal agencies, Chief AI Officers, agency leadership, AI governance boards. Indirectly affects any entity receiving federal funding or contracting with federal government.</p>
  <p><strong>Enforcement:</strong> Binding on federal agencies. Agencies must report compliance to OMB. Non-compliant high-impact AI use cases must be discontinued. Waivers require written determination and reporting within 30 days.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Chief AI Officers (CAIOs) lead agency AI governance, risk management, and strategic adoption. AI Governance Boards provide cross-functional oversight with IT, cybersecurity, data, and budget representation. Agency heads retain discretion on implementation.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Cross-functional governance boards required for CFO Act agencies. Workforce development and employee upskilling emphasized. Public transparency through annual AI use case inventories.</td></tr>
    <tr><td>Accountability</td><td>Annual AI use case inventory publicly published. Risk determinations and waivers reported with justifications. Non-compliant high-impact use cases must be discontinued. OMB oversight of agency compliance.</td></tr>
    <tr><td>Remedy Process</td><td>Appeals and remedies required for individuals affected by high-impact AI decisions. Human oversight with intervention safeguards. Agencies must pause or cease AI use if compliance requirements aren't met.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Adequate human oversight required for high-impact AI with intervention safeguards. Human-in-the-loop maintained for consequential decisions.</td></tr>
    <tr><td>Transparency</td><td>Annual public inventory of AI use cases. Public reporting of risk determinations and waivers with justifications. Increased transparency emphasized as streamlining goal.</td></tr>
    <tr><td>Non-Discrimination</td><td>High-impact AI definition covers systems affecting civil rights and civil liberties. Safeguards for civil rights, civil liberties, and privacy maintained. Appeals/remedies for affected individuals.</td></tr>
    <tr><td>Data Governance</td><td>Pre-deployment testing for risks and benefits. AI Impact Assessments before and during deployment.</td></tr>
    <tr><td>Risk Assessment</td><td>Risk-based approach focusing on high-impact AI. Pre-deployment testing required. Ongoing AI Impact Assessments. Risk determinations must be documented and reported.</td></tr>
  </table>

  <h3>High-Impact AI Definition</h3>
  <div class="highlight-box">
    <strong>General Definition:</strong> AI whose output serves as a 'principal basis' for decisions or actions that have a legal, material, binding, or significant effect on rights or safety.
  </div>
  <p><strong>Covered Areas:</strong></p>
  <ul>
    <li>Civil rights and civil liberties</li>
    <li class="education-highlight" style="list-style: none; margin-left: -20px; padding: 4px 8px;"><strong style="font-size: 11pt;">&#9658; ACCESS TO EDUCATION</strong></li>
    <li>Housing</li>
    <li>Insurance</li>
    <li>Credit</li>
    <li>Employment</li>
    <li>Government programs and services</li>
    <li>Human health and safety</li>
    <li>Critical infrastructure</li>
    <li>Strategic assets</li>
  </ul>
  <div class="critical">
    <strong>Education Relevance:</strong> EXPLICITLY includes 'access to education' as high-impact category, meaning federal AI affecting educational access triggers minimum risk management practices.
  </div>

  <h3>Minimum Risk Practices for High-Impact AI</h3>
  <ol>
    <li>Pre-deployment testing for risks and benefits</li>
    <li>AI Impact Assessments before and during deployment</li>
    <li>Adequate human oversight with intervention safeguards</li>
    <li>Appeals and remedies for affected individuals</li>
    <li>Pause or cease use if compliance requirements aren't met</li>
  </ol>

  <h3>Agency Requirements Timeline</h3>
  <table>
    <tr><th>Deadline</th><th>Requirement</th></tr>
    <tr><td><strong>60 Days</strong></td><td>Designate Chief AI Officers to lead governance, risk management, and strategic AI adoption</td></tr>
    <tr><td><strong>90 Days</strong></td><td>CFO Act agencies establish AI Governance Boards for cross-functional oversight</td></tr>
    <tr><td><strong>180 Days</strong></td><td>Streamline AI adoption by reducing unnecessary requirements, increasing transparency, maximizing existing resources</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Current binding federal AI governance policy (April 2025). Explicitly includes 'access to education' in high-impact AI definition. Creates Chief AI Officer and AI Governance Board requirements that may become models for other institutions. Risk-based approach with defined minimum practices. Public transparency requirements establish accountability model.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Federal policy affects federally-funded educational institutions
  </div>
  <ul class="relevant-elements">
    <li><strong>EDUCATION EXPLICITLY LISTED</strong> as high-impact AI category</li>
    <li>Chief AI Officer model adaptable to institutional governance</li>
    <li>AI Governance Board structure applicable to college governance</li>
    <li>Minimum risk practices provide baseline for institutional policy</li>
    <li>Public inventory requirement models transparency</li>
    <li>Appeals/remedies requirement relevant to student rights</li>
    <li>Waiver process model for institutional flexibility</li>
    <li>Pilot program exemptions applicable to educational innovation</li>
  </ul>
  <p><em>Adaptation Needed:</em> Policy applies to federal agencies, not directly to educational institutions. However: (1) Federally-funded institutions may face compliance pressure, (2) Federal contractors/grantees may need to demonstrate alignment, (3) Structure provides model for institutional AI governance, (4) 'Access to education' definition may inform state/accreditor requirements. Community colleges should monitor for downstream regulatory effects.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 8: GAO -->
<!-- ============================================================ -->
<div class="framework-card" id="gao">
  <div class="framework-header">8. Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> U.S. Government Accountability Office (GAO)</span>
    <span><strong>Year:</strong> 2021</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">National</span>
    <span><strong>Geographic Focus:</strong> United States (applicable beyond federal agencies)</span>
    <span><strong>Document Link</strong><a href="https://www.gao.gov/products/gao-21-519sp"> Document Source</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide key accountability practices to help federal agencies and other entities use AI responsibly. Enable managers to ensure accountability in government programs and processes using AI. Serve as practical, actionable blueprint for evaluating and auditing AI systems.</p>
  <p><strong>Key Principles:</strong> Four complementary principles: Governance, Data, Performance, and Monitoring. Organized around complete AI system lifecycle (Design, Development, Deployment, Monitoring). Human-centered approach recognizing AI operates within broader contexts.</p>
  <p><strong>Target Audience:</strong> Federal agencies, inspectors general, legal counsels, compliance professionals, third-party assessors, auditors, industry, academia, and nonprofits. Designed to enable non-experts to ask the right questions about AI systems.</p>
  <p><strong>Enforcement:</strong> Framework for audit and accountability rather than binding regulation. Provides benchmark for assessing safety, fairness, and effectiveness. Independent verification emphasized as essential.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Framework supports existing governance structures with accountability overlay. Emphasizes oversight and decision-making processes for AI implementation.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Developed through Comptroller General Forum with AI experts from federal government, industry, and nonprofit sectors. Extensive literature review and independent validation from program officials and subject matter experts.</td></tr>
    <tr><td>Accountability</td><td>Each principle includes questions for entities, auditors, and third-party assessors. Audit procedures and evidence types specified. Independent verification required for bias detection.</td></tr>
    <tr><td>Remedy Process</td><td>Monitoring principle includes traceability for corrective actions. Framework addresses detecting and responding to model drift.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Human-centered approach recognizing AI operates within broader contexts. Human supervision procedures required. Framework addresses oversight from design through deployment.</td></tr>
    <tr><td>Transparency</td><td>Framework evaluates reasoning behind chosen transparency mechanisms. Does not prescribe specific methods (scorecards, data cards, nutrition labels) but assesses adequacy for use case.</td></tr>
    <tr><td>Non-Discrimination</td><td>Bias detection and mitigation addressed. Independent verification emphasized as only way to verify AI is not biased. Data representativeness evaluation required.</td></tr>
    <tr><td>Data Governance</td><td>Data principle covers quality, reliability, and representativeness. Security considerations included. Data processing assessment required.</td></tr>
    <tr><td>Risk Assessment</td><td>Performance metrics for safety and fairness. Model drift detection. Assessment of AI system expansion risks.</td></tr>
  </table>

  <h3>Four Principles</h3>
  <div class="principle-box">
    <strong>Principle 1: Governance</strong><br>
    Promote accountability by establishing processes to manage, operate, and oversee AI implementation.
    <ul style="margin-bottom: 0;">
      <li>Establish clear goals for AI implementation</li>
      <li>Engage with diverse stakeholders</li>
      <li>Recruit, develop, and retain personnel with multidisciplinary skills</li>
      <li>Establish human supervision procedures</li>
      <li>Create oversight structures for AI decision-making</li>
    </ul>
  </div>
  <div class="principle-box">
    <strong>Principle 2: Data</strong><br>
    Ensure quality, reliability, and representativeness of data sources and processing.
    <ul style="margin-bottom: 0;">
      <li>Use reliable data sources</li>
      <li>Evaluate data representativeness</li>
      <li>Assess data quality and integrity</li>
      <li>Address data bias</li>
      <li>Ensure appropriate data processing</li>
    </ul>
  </div>
  <div class="principle-box">
    <strong>Principle 3: Performance</strong><br>
    Produce results that are consistent with program objectives.
    <ul style="margin-bottom: 0;">
      <li>Establish performance metrics</li>
      <li>Assess AI system effectiveness</li>
      <li>Evaluate consistency with objectives</li>
      <li>Test for intended outcomes</li>
      <li>Measure safety and fairness</li>
    </ul>
  </div>
  <div class="principle-box">
    <strong>Principle 4: Monitoring</strong><br>
    Ensure AI systems remain reliable and relevant over time.
    <ul style="margin-bottom: 0;">
      <li>Detect model drift</li>
      <li>Document results of monitoring activities</li>
      <li>Implement corrective actions (traceability)</li>
      <li>Assess AI system expansion or growth</li>
      <li>Evaluate transfer between application spaces</li>
    </ul>
  </div>

  <h3>Audit Provisions</h3>
  <table>
    <tr><th>Aspect</th><th>Details</th></tr>
    <tr><td>Structure</td><td>Each practice includes questions for entities, auditors, and third-party assessors, plus audit procedures and evidence types to collect</td></tr>
    <tr><td>Auditor Access Rights</td><td>Framework requires auditors retain rights to audit AI systems, inspect models, and access underlying data</td></tr>
    <tr><td>IP Protection</td><td>Protects oversight capability from vendor intellectual property claims</td></tr>
    <tr><td>Independent Verification</td><td>Emphasized as essential for bias detection &mdash; 'the only way to verify your AI is not biased'</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First comprehensive AI accountability framework from federal audit authority. Identifies 31 key practices across four principles. Flexible enough to adapt to evolving technologies. Enables non-experts to ask the right questions. Establishes benchmark for assessing safety, fairness, and effectiveness. Human-centered approach distinguishes from purely technical frameworks. Protects auditor access rights against vendor IP claims. Nearly 50 GAO AI products since 2018 demonstrate ongoing application.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Audit framework applicable to any institution using AI
  </div>
  <ul class="relevant-elements">
    <li>Four-principle structure adaptable to institutional AI governance</li>
    <li>Audit questions applicable to internal AI assessment</li>
    <li>Governance principle provides oversight structure model</li>
    <li>Data principle addresses educational data quality concerns</li>
    <li>Performance principle supports learning outcome measurement</li>
    <li>Monitoring principle ensures ongoing AI system reliability</li>
    <li>Independent verification model for vendor AI assessment</li>
    <li>Bias detection methodology applicable to student-facing AI</li>
    <li>31 key practices provide comprehensive checklist</li>
    <li>Non-expert accessibility supports faculty/staff engagement</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework designed for federal context but principles transfer to educational institutions. Adaptations needed for: (1) Educational privacy laws (FERPA), (2) Academic governance structures, (3) Vendor relationship dynamics in EdTech, (4) Student outcome measurement approaches. Framework's flexibility supports adaptation to CC context.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 9: NIST AI 600-1 -->
<!-- ============================================================ -->
<div class="framework-card" id="nist">
  <div class="framework-header">9. NIST AI 600-1: Artificial Intelligence Risk Management Framework - Generative Artificial Intelligence Profile</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> National Institute of Standards and Technology (NIST)</span>
    <span><strong>Year:</strong> 2024</span>
    <span class="meta-tag">Standard</span>
    <span class="meta-tag">National</span>
    <span><strong>Geographic Focus:</strong> United States (voluntary, cross-sectoral)</span>
    <span><strong>Document Link</strong><a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf"> Download Link</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Help organizations identify unique risks posed by generative AI and propose actions for risk management aligned with their goals and priorities. Provide cross-sectoral profile of AI RMF 1.0 specifically for Generative AI technologies. Support implementation of Executive Order 14110 on Safe, Secure, and Trustworthy AI.</p>
  <p><strong>Key Principles:</strong> Risk-based approach tailored to generative AI. Trustworthiness considerations in AI lifecycle. Voluntary adoption with flexible implementation. Four primary considerations: Governance, Content Provenance, Pre-deployment Testing, and Incident Disclosure.</p>
  <p><strong>Target Audience:</strong> Organizations developing, deploying, or using generative AI systems. AI developers, deployers, and users across all sectors. Risk managers and governance professionals.</p>
  <p><strong>Enforcement:</strong> Voluntary framework. Not legally required but increasingly referenced in procurement and compliance contexts.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Organizations implement based on their risk tolerance and resources. Framework provides structure; organizations determine application.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Generative AI Public Working Group (GAI PWG) contributed to development. Public comment period incorporated broad input.</td></tr>
    <tr><td>Accountability</td><td>Self-assessment against framework categories. Documentation of risk management decisions. Incident disclosure processes.</td></tr>
    <tr><td>Remedy Process</td><td>Framework addresses incident response and disclosure. Continuous monitoring enables corrective action.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Human-AI configuration addressed as specific risk area. Proper oversight structures recommended. Human review of high-stakes outputs emphasized.</td></tr>
    <tr><td>Transparency</td><td>Content provenance as primary consideration. Synthetic content identification requirements. Disclosure of AI involvement in content generation.</td></tr>
    <tr><td>Non-Discrimination</td><td>Harmful bias identified as specific risk. Bias assessment in pre-deployment testing. Mitigation actions for discriminatory outputs.</td></tr>
    <tr><td>Data Governance</td><td>Data privacy as specific risk category. Training data provenance tracking. Purpose specification for data use.</td></tr>
    <tr><td>Risk Assessment</td><td>Twelve-risk taxonomy provides assessment structure. Pre-deployment testing requirements. Ongoing monitoring for emerging risks.</td></tr>
  </table>

  <h3>Four Primary Considerations</h3>
  <div class="area-box">
    <strong>Governance:</strong> Organizational structures and processes for GAI risk management. Focus areas: Accountability structures, Policies and procedures, Roles and responsibilities, Resource allocation.
  </div>
  <div class="area-box">
    <strong>Content Provenance:</strong> Tracking and documenting origin and history of AI-generated content. Focus areas: Synthetic content identification, Watermarking and labeling, Disclosure of AI involvement, Chain of custody for training data.
  </div>
  <div class="area-box">
    <strong>Pre-deployment Testing:</strong> Evaluation of GAI systems before release. Focus areas: Safety testing, Bias assessment, Security evaluation, Performance benchmarking.
  </div>
  <div class="area-box">
    <strong>Incident Disclosure:</strong> Processes for reporting and responding to GAI incidents. Focus areas: Incident identification, Reporting mechanisms, Response procedures, Lessons learned integration.
  </div>

  <h3>Twelve GAI Risks</h3>
  <table>
    <tr><th style="width: 5%">#</th><th style="width: 25%">Risk</th><th>Description</th></tr>
    <tr><td>1</td><td>CBRN Information Access</td><td>Accessing materially nefarious information or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) weapons of mass destruction (WMD) or other dangerous materials or agents</td></tr>
    <tr><td>2</td><td>Confabulation/Hallucination</td><td>Creating content that is confabulation, hallucination, or fabrication</td></tr>
    <tr><td>3</td><td>Dangerous or Violent Recommendations</td><td>Generating content that facilitates dangerous or violent actions</td></tr>
    <tr><td>4</td><td>Data Privacy</td><td>Risks to privacy principles including transparency, individual participation, consent, and purpose specification</td></tr>
    <tr><td>5</td><td>Environmental Impacts</td><td>Negative environmental effects from GAI training and deployment</td></tr>
    <tr><td>6</td><td>Harmful Bias/Homogenization</td><td>Perpetuating or amplifying harmful biases or creating homogenized outputs</td></tr>
    <tr><td>7</td><td>Human-AI Configuration</td><td>Risks from improper human-AI interaction design</td></tr>
    <tr><td>8</td><td>Information Integrity</td><td>Undermining information integrity through synthetic content</td></tr>
    <tr><td>9</td><td>Information Security</td><td>Security vulnerabilities in GAI systems</td></tr>
    <tr><td>10</td><td>Intellectual Property</td><td>Risks related to training data and output IP concerns</td></tr>
    <tr><td>11</td><td>Obscene/Degrading Content</td><td>Generation of obscene, degrading, or abusive content</td></tr>
    <tr><td>12</td><td>Value Chain/Component Integration</td><td>Risks from GAI component integration and supply chain</td></tr>
  </table>
  <p style="font-size: 10pt; color: #666;">Each risk has associated mitigation actions mapped to AI RMF functions.</p>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First NIST framework specifically addressing generative AI risks. Twelve-risk taxonomy provides comprehensive risk identification. Content provenance focus addresses synthetic content challenges. Builds on established AI RMF with GAI-specific guidance. Pre-deployment testing emphasis supports responsible development. Incident disclosure framework enables learning across organizations. Voluntary but increasingly influential in procurement and policy.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Directly applicable to institutional GenAI adoption
  </div>
  <ul class="relevant-elements">
    <li>Twelve-risk taxonomy applicable to educational GenAI assessment</li>
    <li>Confabulation/hallucination risk critical for tutoring and advising AI</li>
    <li>Content provenance for academic integrity</li>
    <li>Data privacy provisions align with FERPA concerns</li>
    <li>Harmful bias risk for student-facing AI</li>
    <li>Pre-deployment testing model for EdTech evaluation</li>
    <li>Human-AI configuration for faculty/student AI interaction design</li>
    <li>Intellectual property risk for student work and institutional content</li>
    <li>Incident disclosure model for AI failure reporting</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework is general-purpose; CC adaptation requires: (1) Educational context for risk definitions (e.g., hallucination in tutoring), (2) FERPA-specific privacy considerations, (3) Academic integrity implications of content provenance, (4) Student-appropriate human-AI configuration, (5) Educational outcome metrics for performance assessment. Framework's voluntary nature suits institutional flexibility.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 10: NAIC -->
<!-- ============================================================ -->
<div class="framework-card" id="naic">
  <div class="framework-header">10. NAIC Model Bulletin on the Use of Artificial Intelligence Systems by Insurers</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> National Association of Insurance Commissioners (NAIC)</span>
    <span><strong>Year:</strong> 2023 (adopted), 2024-2025 (state implementation)</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States (state-by-state adoption)</span>
    <span><strong>Document Link</strong><a href="NAIC Model Bulletin on the Use of Artificial Intelligence Systems by Insurers"> Download Link</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Set state insurance regulators' expectations for how insurers should govern AI use. Ensure AI-supported decisions impacting consumers are accurate and comply with unfair trade practice laws. Create framework for responsible AI use in insurance while enabling beneficial applications.</p>
  <p><strong>Key Principles:</strong> Risk-based governance commensurate with AI risk assessment. Consumer protection in AI-driven decisions. Transparency to consumers about AI use. Third-party vendor accountability. Insurer responsibility regardless of whether AI is internally developed or procured.</p>
  <p><strong>Target Audience:</strong> Insurance companies, third-party AI vendors serving insurers, state insurance regulators, compliance professionals.</p>
  <p><strong>Enforcement:</strong> State-level adoption required for enforcement. 24 states and districts adopted as of March 2025. Market conduct examinations and regulatory inquiries enforce requirements.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Insurers must establish governance structure with stakeholders from actuarial, data science, underwriting, compliance, and legal departments. Each representative has defined responsibilities, authority, and decision-making powers.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Cross-functional governance required. Consumer notification when AI systems are in use. Industry input through NAIC committee process.</td></tr>
    <tr><td>Accountability</td><td>Written AIS Program required documenting responsible AI practices. Market conduct examinations may request governance documentation. Regulatory inquiries into AI practices authorized.</td></tr>
    <tr><td>Remedy Process</td><td>State insurance regulatory complaint processes. Market conduct actions for non-compliance. Examination findings can require remediation.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Human review required for consequential AI-driven decisions. Governance structure must include human decision-makers with authority. Not fully specified but implied through accountability requirements.</td></tr>
    <tr><td>Transparency</td><td>Insurers must notify consumers when AI systems are in use. Consumers should have access to appropriate information about how AI may affect decisions impacting them. Level of information may vary by insurance lifecycle phase.</td></tr>
    <tr><td>Non-Discrimination</td><td>AI decisions must not violate unfair trade practice laws. Bias prevention emphasized. Iowa's definition of bias provides operational clarity. Decisions must be accurate and non-discriminatory.</td></tr>
    <tr><td>Data Governance</td><td>Data used in AI systems must be assessed for quality and representativeness. Third-party data sources must be evaluated. Audit rights over vendor data required.</td></tr>
    <tr><td>Risk Assessment</td><td>Risk assessment required in accordance with NAIC 2020 Principles of Artificial Intelligence. Risk-based approach to governance intensity. Continuous monitoring of AI system performance.</td></tr>
  </table>

  <h3>AIS Program Requirements</h3>
  <div class="highlight-box">
    <strong>Written AIS Program Required:</strong> Insurers must create written program ('AIS Program') detailing responsible use of AI systems, especially when AI is used in decision-making processes impacting regulated insurance practices.
  </div>
  <p><strong>Required Elements:</strong></p>
  <ul>
    <li>Purpose, scope, and structure of AI systems in decision-making</li>
    <li>Policies and procedures for responsible AI use</li>
    <li>Risk management framework</li>
    <li>Internal controls</li>
    <li>Governance structure with defined responsibilities</li>
  </ul>
  <p><strong>Lifecycle Coverage:</strong></p>
  <ul>
    <li>Product development and design</li>
    <li>Marketing</li>
    <li>Underwriting</li>
    <li>Rating and pricing</li>
    <li>Case management</li>
    <li>Claim administration and payment</li>
    <li>Fraud detection</li>
  </ul>

  <h3>Third-Party Vendor Requirements</h3>
  <div class="area-box">
    <strong>Principle:</strong> Insurers responsible for AI regardless of whether internally developed or procured from vendors.
  </div>
  <ul>
    <li>Strong third-party vendor management practices</li>
    <li>Oversight of vendors involved in AI development, procurement, and implementation</li>
    <li>Assessment of data and AI systems provided by third parties</li>
    <li>Contractual protections including audit rights</li>
    <li>Cooperation with regulatory inquiries</li>
  </ul>

  <h3>Adoption Status</h3>
  <table>
    <tr><th>Status</th><th>Details</th></tr>
    <tr><td>Total Adopted</td><td>24 states and districts as of March 2025</td></tr>
    <tr><td>Connecticut</td><td>Bulletin MC-25 (February 26, 2024)</td></tr>
    <tr><td>Pennsylvania</td><td>Notice 2024-04 (April 6, 2024)</td></tr>
    <tr><td>Rhode Island</td><td>Bulletin 2024-03 (March 15, 2024)</td></tr>
    <tr><td>Vermont</td><td>Bulletin 229 (March 12, 2024)</td></tr>
    <tr><td>West Virginia</td><td>Bulletin 24-06 (August 9, 2024)</td></tr>
    <tr><td>Iowa</td><td>First state to define 'bias' as 'a distortion or error in statistical analysis that produces inaccurate results' (November 2024)</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First comprehensive US regulatory framework for AI in a major industry sector. Provides 'executable ethics' model with operational requirements. Third-party vendor accountability model applicable across sectors. Consumer notification requirement sets transparency precedent. State-by-state adoption creates implementation laboratories. Risk-based governance approach with defined documentation requirements. Regulatory examination authority ensures accountability.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Model for sector-specific AI governance with operational requirements
  </div>
  <ul class="relevant-elements">
    <li>Written AI governance program requirement adaptable to institutional policy</li>
    <li>Third-party vendor accountability model directly applicable to EdTech vendors</li>
    <li>Consumer notification requirement translatable to student notification</li>
    <li>Cross-functional governance structure model</li>
    <li>Risk-based approach to governance intensity</li>
    <li>Documentation requirements for regulatory/accreditation review</li>
    <li>Lifecycle coverage model (enrollment, advising, assessment, completion)</li>
    <li>Bias definition from Iowa applicable to educational AI</li>
    <li>Audit rights over vendor AI applicable to EdTech contracts</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework designed for insurance; CC adaptation requires: (1) Translation from consumer to student context, (2) Modification of lifecycle phases to educational journey, (3) Alignment with FERPA rather than insurance privacy laws, (4) Accreditation rather than market conduct examination, (5) Academic rather than actuarial expertise in governance. Model provides excellent template for operational AI governance in CC context.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 11: LOCAL GOVERNMENT HANDBOOK -->
<!-- ============================================================ -->
<div class="framework-card" id="local-gov">
  <div class="framework-header">11. Artificial Intelligence Handbook for Local Government</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> University of Michigan STPP / Michigan Municipal League</span>
    <span><strong>Year:</strong> 2024</span>
    <span class="meta-tag">Toolkit</span>
    <span class="meta-tag">National</span>
    <span><strong>Geographic Focus:</strong> United States (developed for Michigan, applicable nationally)</span>
    <span><strong>Document Link</strong><a href="https://stpp.fordschool.umich.edu/research/community-resource/artificial-intelligence-handbook-local-government"> Download Link</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Fill the guidance gap for resource-constrained local governments lacking resources to harness AI safely and effectively. Provide concrete guidelines, best practices, sample applications, and risk assessment strategies to kickstart safe AI adoption. Address the needs of small municipalities (10,000-20,000 population) being approached by AI vendors without evaluation guidance.</p>
  <p><strong>Key Principles:</strong> Ethical, responsible, and legally compliant AI use consistent with US guidelines as of 2024. Practical accessibility over technical jargon. Addresses real municipal challenges rather than theoretical frameworks. Acknowledges AI as rapidly evolving field requiring adaptable principles.</p>
  <p><strong>Target Audience:</strong> Mayors, city officials, municipal administrators, and local government staff in resource-constrained municipalities. Designed for non-technical audiences.</p>
  <p><strong>Enforcement:</strong> Voluntary guidelines. Consistent with ethical, responsible, and emerging legal guidelines in the US as of 2024.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Local government leadership-driven. Provides evaluation framework for municipal decision-makers assessing AI proposals from vendors.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Developed through collaboration between University of Michigan Ford School of Public Policy, U-M Engineering Department, current local officials, and Michigan Municipal League associates. Based on interviews with local officials to identify primary concerns.</td></tr>
    <tr><td>Accountability</td><td>Self-assessment tools including AI evaluation checklist. Risk assessment strategies for ongoing monitoring.</td></tr>
    <tr><td>Remedy Process</td><td>Guidelines for identifying and addressing AI-related concerns at local level.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Emphasis on human evaluation of AI vendor proposals. Municipal officials positioned as decision-makers rather than passive technology recipients.</td></tr>
    <tr><td>Transparency</td><td>Designed to help officials ask the right questions of AI vendors. Accessible language avoiding jargon and mathematical complexity.</td></tr>
    <tr><td>Non-Discrimination</td><td>Ethical guidelines aligned with US responsible AI principles. Addresses equity implications for municipal service delivery.</td></tr>
    <tr><td>Data Governance</td><td>Guidance on data considerations in AI deployment for public services.</td></tr>
    <tr><td>Risk Assessment</td><td>Dedicated risk assessment strategies. AI evaluation checklist for assessing proposed AI tools.</td></tr>
  </table>

  <h3>Handbook Structure</h3>
  <div class="area-box">
    <strong>Section 1:</strong> AI Guidelines for Local Government
  </div>
  <div class="area-box">
    <strong>Section 2:</strong> Best Practices for Using Generative AI Tools (ChatGPT, Copilot, Gemini)
  </div>
  <div class="area-box">
    <strong>Section 3:</strong> Checklist for Assessing AI
  </div>
  <div class="area-box">
    <strong>Section 4:</strong> Sample Applications
  </div>
  <div class="area-box">
    <strong>Section 5:</strong> Risk Assessment Strategies
  </div>
  <p style="font-size: 10pt; color: #666;">Development methodology: Researched existing guidelines from cities (Boston, Seattle) and 15+ other city, state, and national governments. Conducted interviews with local officials to identify primary concerns. Collaborated with U-M computer scientists for technical accuracy while maintaining readability.</p>

  <h3>Unique Contribution</h3>
  <div class="callout">
    One of the few AI resources specifically tailored to cities and small municipalities. Addresses the gap where federal regulations target AI developers or large federal agencies but not local governments. Designed for resource-constrained organizations without dedicated technical staff &mdash; directly parallels community college context. Plain-language approach makes technical concepts accessible to non-expert audiences. Practical vendor evaluation guidance fills critical need.
  </div>

  <h3>CC Relevance</h3>
  <div class="callout relevance-critical" style="padding: 12px 16px;">
    <strong>Applicability: Critical</strong> &mdash; Resource-constrained institution model directly applicable to community colleges
  </div>
  <ul class="relevant-elements">
    <li>Designed for organizations without dedicated AI/compliance teams</li>
    <li>Vendor evaluation guidance applicable to EdTech procurement</li>
    <li>Risk assessment checklist adaptable to educational context</li>
    <li>Plain-language accessibility model for governance documents</li>
    <li>Best practices for generative AI tools (ChatGPT, etc.)</li>
    <li>Synthesizes 15+ government AI guidelines into practical format</li>
    <li>Addresses small organization capacity constraints</li>
    <li>Decision-maker empowerment approach</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework designed for municipalities but principles transfer to educational institutions. Adaptations needed for: (1) Educational privacy laws (FERPA vs. municipal records), (2) Student-specific AI applications, (3) Academic integrity considerations, (4) Faculty governance structures, (5) Accreditation requirements. Core risk assessment and vendor evaluation approaches directly applicable.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 12: SINGAPORE AGENTIC AI -->
<!-- ============================================================ -->
<div class="framework-card" id="singapore">
  <div class="framework-header">12. Singapore Model AI Governance Framework for Agentic AI (MGF for Agentic AI) and Securing Agentic AI Addendum</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> IMDA / CSA / GovTech (Singapore)</span>
    <span><strong>Year:</strong> 2025-2026</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">National</span>
    <span><strong>Geographic Focus:</strong> Singapore (intended as global reference)</span>
    <span><strong>Document Link</strong><a href="https://www.imda.gov.sg/-/media/imda/files/about/emerging-tech-and-research/artificial-intelligence/mgf-for-agentic-ai.pdf"> PDF Document Link</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide first-of-its-kind governance framework for reliable and safe agentic AI deployment. Address unique risks of AI systems capable of autonomous decision-making and goal-setting. Build on Singapore's 2020 Model AI Governance Framework foundation. Serve as invitation to governments, researchers, and industry partners to help shape global reference for agentic AI governance.</p>
  <p><strong>Key Principles:</strong> Humans remain ultimately accountable for agent actions. Risk-based approach to bounding agent autonomy. Technical controls throughout agent lifecycle. End-user responsibility through transparency and education. Balance innovation enablement with risk mitigation.</p>
  <p><strong>Target Audience:</strong> Organizations deploying AI agents, system owners, developers, end-users integrating agents into workflows, and policymakers globally.</p>
  <p><strong>Enforcement:</strong> Voluntary framework. Living document welcoming feedback and case studies. Public consultation (October 22 &ndash; December 31, 2025 for CSA Addendum).</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Organizations define responsibility chains across teams. Significant checkpoints requiring human approval before sensitive actions. Clear accountability from design through deployment.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Developed by CSA, GovTech, and IMDA in collaboration with industry and professional partners. Open feedback process. Announced at World Economic Forum and Singapore International Cyber Week.</td></tr>
    <tr><td>Accountability</td><td>Agents require unique identities linked to supervising entities. Logging of all tool usage. Human approval mandated for high-stakes decisions. Permissions restricted to approved functions.</td></tr>
    <tr><td>Remedy Process</td><td>Anomaly detection and threshold-based alerting. Real-time intervention capabilities. Checkpoints for irreversible actions and atypical behavior patterns.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Human-in-the-loop for high-stakes decisions. Human approval checkpoints for sensitive actions. Addresses automation bias risk. Real-time intervention capabilities maintained.</td></tr>
    <tr><td>Transparency</td><td>End-user transparency on agent capabilities and data usage. Logging of all tool usage. Escalation contacts provided. Training requirements for users.</td></tr>
    <tr><td>Non-Discrimination</td><td>Risk assessment includes task sensitivity evaluation. Framework applicable across different scenarios and use cases.</td></tr>
    <tr><td>Data Governance</td><td>Limits on agent data access. Permissions restricted to approved functions. Unique agent identities linked to supervising entities.</td></tr>
    <tr><td>Risk Assessment</td><td>Capability-based risk framing for agentic systems. Workflow mapping to identify autonomy-related risk points. Risk factors: task sensitivity, data criticality, action reversibility.</td></tr>
  </table>

  <h3>Four Key Areas</h3>
  <div class="area-box">
    <strong>Area 1: Assessing and Bounding Risks Upfront</strong><br>
    Selecting appropriate agentic use cases and placing limits on agents' powers.
    <ul style="margin-bottom: 0;">
      <li>Evaluate suitable use cases for agentic AI</li>
      <li>Establish limits on agent autonomy</li>
      <li>Restrict data access and tool permissions</li>
      <li>Identify risk factors: task sensitivity, data criticality, reversibility of actions</li>
      <li>Map agentic workflows to identify vulnerability points</li>
    </ul>
  </div>
  <div class="area-box">
    <strong>Area 2: Making Humans Meaningfully Accountable</strong><br>
    Ensuring humans remain ultimately accountable for agent actions despite automation.
    <ul style="margin-bottom: 0;">
      <li>Define significant checkpoints requiring human approval</li>
      <li>Establish clear responsibility chains across teams</li>
      <li>Address automation bias (over-trusting reliable systems)</li>
      <li>Immediate steps to make humans meaningfully accountable post-deployment</li>
      <li>Define responsibility across multiple actors within and outside organization</li>
    </ul>
  </div>
  <div class="area-box">
    <strong>Area 3: Implementing Technical Controls and Processes</strong><br>
    Controls throughout the agent lifecycle from design through deployment.
    <ul style="margin-bottom: 0;">
      <li>Design-phase security measures</li>
      <li>Baseline testing for safety and robustness</li>
      <li>Pre-deployment testing</li>
      <li>Continuous post-deployment monitoring</li>
      <li>Real-time intervention capabilities</li>
      <li>Access control to whitelisted services</li>
      <li>Scenario-based testing for different autonomy levels</li>
    </ul>
  </div>
  <div class="area-box">
    <strong>Area 4: Enabling End-User Responsibility</strong><br>
    Transparency and education for those integrating agents into workflows.
    <ul style="margin-bottom: 0;">
      <li>Transparency regarding agent capabilities</li>
      <li>Clear communication of data usage</li>
      <li>Escalation contacts provided</li>
      <li>Training on common failure modes</li>
      <li>Guidance on proper oversight practices</li>
    </ul>
  </div>

  <h3>Identity and Monitoring Provisions</h3>
  <table>
    <tr><th>Aspect</th><th>Details</th></tr>
    <tr><td>Agent Identity</td><td>Agents require unique identities linked to supervising entities</td></tr>
    <tr><td>Permissions</td><td>Restricted to approved functions</td></tr>
    <tr><td>Logging</td><td>All tool usage logged</td></tr>
    <tr><td>Monitoring</td><td>Anomaly detection and threshold-based alerting</td></tr>
    <tr><td>Checkpoints</td><td>Human review for high-stakes decisions, irreversible actions, atypical behavior</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First authoritative government framework specifically addressing agentic AI governance. Fills critical gap in policy guidance for autonomous AI agents. Distinguishes agentic systems from assistive models. Addresses unique risks: unauthorized/erroneous actions, automation bias, expanded attack surface. Living document approach acknowledges rapid evolution. Intended as global reference model. Practical examples across different autonomy levels.
  </div>

  <h3>CC Relevance</h3>
  <div class="callout relevance-critical" style="padding: 12px 16px;">
    <strong>Applicability: Critical</strong> &mdash; Forward-looking for tutoring agents, advising bots, automated student support
  </div>
  <ul class="relevant-elements">
    <li>Four-area framework applicable to educational AI agents</li>
    <li>Human accountability model for student-facing autonomous systems</li>
    <li>Risk assessment approach for tutoring and advising agents</li>
    <li>Transparency requirements for student interactions with AI</li>
    <li>Technical controls for educational agent lifecycle</li>
    <li>End-user training model for faculty and staff</li>
    <li>Checkpoint approach for sensitive student decisions</li>
    <li>Workflow mapping for educational agent vulnerabilities</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework is general-purpose; educational context requires adaptation for: (1) Student privacy (FERPA), (2) Academic integrity in agent-assisted work, (3) Faculty oversight of tutoring agents, (4) Advising agent boundaries and escalation, (5) Student vulnerability considerations, (6) Educational outcome measurement. Singapore model provides structure for CC agentic AI policy development.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 13: UNESCO INDIGENOUS -->
<!-- ============================================================ -->
<div class="framework-card" id="unesco-indigenous">
  <div class="framework-header">13. UNESCO Guidelines for Indigenous Data Sovereignty in Artificial Intelligence Developments</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> UNESCO</span>
    <span><strong>Year:</strong> 2024-2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">International</span>
    <span><strong>Geographic Focus:</strong> Global (initial focus on Latin America and Caribbean)</span>
    <span><strong>Document Link</strong><a href="unesdoc.unesco.org/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_89f3e544-fcc0-4a41-af0e-5825b7d0cf76?_=387814spa.pdf&to=53&from=1"> Download Link(Spanish)</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Foster culturally sensitive technological development and information dissemination. Ensure data sovereignty of over 800 Indigenous Peoples in Latin America and the Caribbean. Prevent inappropriate cultural adoption within accelerated AI development. Integrate Indigenous perspectives in all phases of AI development.</p>
  <p><strong>Key Principles:</strong> Indigenous data sovereignty. Free, Prior and Informed Consent (FPIC). Community-Based Participatory Research. Human-centric technological approach. Rights of Indigenous peoples to collect, interpret, and use their voices, images, representations, knowledge, techniques, symbolic and linguistic systems.</p>
  <p><strong>Target Audience:</strong> Governments, AI developers, researchers, Indigenous communities, policymakers implementing UNESCO AI ethics recommendations.</p>
  <p><strong>Enforcement:</strong> Voluntary guidance implementing UNESCO Recommendation on Ethics of AI. Tied to member state commitments to cultural and linguistic diversity.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Participatory inclusion of local and Indigenous communities in AI governance. Indigenous peoples' right to determine how their data is collected, interpreted, and used.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Indigenous communities as primary stakeholders with decision-making authority over their data. Community-Based Participatory Research model. Multi-stakeholder dialogue including governments, developers, and Indigenous peoples.</td></tr>
    <tr><td>Accountability</td><td>Free, Prior and Informed Consent as foundational requirement. Alignment with UN Declaration on Rights of Indigenous Peoples. Member state reporting on UNESCO Recommendation implementation.</td></tr>
    <tr><td>Remedy Process</td><td>Indigenous community authority to withdraw consent. Collective data governance rights. Appeals to UNESCO oversight mechanisms.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Indigenous community oversight of AI systems using their data. Human-centric approach that centers Indigenous perspectives. Community control over AI applications affecting their peoples.</td></tr>
    <tr><td>Transparency</td><td>Full disclosure of AI system purposes and data uses to Indigenous communities. Clear communication about how Indigenous knowledge, images, and cultural materials are utilized.</td></tr>
    <tr><td>Non-Discrimination</td><td>Protection against inappropriate cultural adoption. Recognition of unique characteristics of Indigenous data. Prevention of AI systems that misrepresent or harm Indigenous peoples.</td></tr>
    <tr><td>Data Governance</td><td>Indigenous Data Sovereignty as core principle &mdash; communities own, control, and govern their data. Proper management of Indigenous data respecting autonomy. Right to determine collection, interpretation, and use of community data.</td></tr>
    <tr><td>Risk Assessment</td><td>Assessment of AI impact on cultural and linguistic diversity. Evaluation of potential for cultural appropriation or misrepresentation. Risk analysis for endangered languages and knowledge systems.</td></tr>
  </table>

  <h3>Core Guidelines</h3>
  <div class="principle-box">
    <strong>Data Sovereignty:</strong> Indigenous communities have right to own, control, and govern their data. Data collected from Indigenous communities must be used in manner aligning with their values and interests. Informed consent crucial, allowing communities say in how data is utilized and shared.
  </div>
  <div class="principle-box">
    <strong>AI Development Phases:</strong> Integrate Indigenous peoples' perspectives in ALL phases of AI development: Design, Data collection, Training, Deployment, Monitoring.
  </div>
  <div class="principle-box">
    <strong>Cultural Protection:</strong> Scope includes voices, images, representations, knowledge, techniques, symbolic systems, linguistic systems. AI advances must not appropriate or misuse Indigenous cultural materials.
  </div>
  <div class="principle-box">
    <strong>Public Policy:</strong> Propose public policies to integrate Indigenous perspectives. Five best practices from Mexico highlighted in initial report.
  </div>

  <h3>Unique Contribution</h3>
  <div class="callout">
    First international guidance specifically addressing Indigenous data sovereignty in AI. Challenges individual consent model with collective governance framework. Extends data sovereignty beyond personal data to cultural knowledge, languages, and representations. Provides implementation pathway for UNESCO AI ethics recommendation in Indigenous contexts. Connects AI governance to cultural heritage protection and linguistic diversity.
  </div>

  <h3>CC Relevance</h3>
  <div class="highlight-box" style="padding: 12px 16px;">
    <strong>Applicability: Medium-High</strong> &mdash; Critical for tribal colleges, Native-serving institutions, and diversity-focused CCs
  </div>
  <ul class="relevant-elements">
    <li>Collective consent model alternative to individual-only consent</li>
    <li>Cultural sensitivity requirements for AI training data</li>
    <li>Community-based participatory approach applicable to CC governance</li>
    <li>Protection of student cultural and linguistic data</li>
    <li>Indigenous student population considerations</li>
    <li>Workforce programs serving Indigenous communities</li>
    <li>Language preservation AI applications</li>
    <li>Cultural competency in AI system design</li>
  </ul>
  <p><em>Adaptation Needed:</em> Framework designed for Indigenous community governance; CC adaptation requires: (1) Consultation with tribal college partners, (2) Consideration of Native American student populations, (3) Cultural data protection in student information systems, (4) Community input mechanisms for Native-serving programs, (5) Recognition of collective as well as individual data rights. Particularly relevant for CCs with significant Indigenous student populations or tribal partnerships.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 14: FREEDOM ONLINE COALITION -->
<!-- ============================================================ -->
<div class="framework-card" id="foc">
  <div class="framework-header">14. Freedom Online Coalition Joint Statement on Artificial Intelligence and Human Rights</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Freedom Online Coalition (FOC)</span>
    <span><strong>Year:</strong> 2024-2025</span>
    <span class="meta-tag">Policy</span>
    <span class="meta-tag">International</span>
    <span><strong>Geographic Focus:</strong> 42 member countries (democratic nations coalition)</span>
    <span><strong>Document Link</strong><a href="https://freedomonlinecoalition.com/wp-content/uploads/2025/06/ONLINE-Joint-Statement-on-Artificial-Intelligence-and-Human-Rights-2025.pdf"> Download PDF</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Protect human rights and fundamental freedoms in AI development and deployment. Address increased scale and urgency of AI-related human rights risks. Promote rights-respecting, human-centric, safe, secure, trustworthy and ethical AI. Build global AI governance grounded in human rights, accountability, and rule of law.</p>
  <p><strong>Key Principles:</strong> Human rights must be respected throughout AI lifecycle. AI governance grounded in accountability and rule of law. AI should serve public good while safeguarding freedoms. Democratic values in AI development and use.</p>
  <p><strong>Target Audience:</strong> FOC member governments, international organizations, private sector AI developers, civil society, all states and stakeholders.</p>
  <p><strong>Enforcement:</strong> Political commitment of 42 member countries. Moral authority through coalition action. Coordination across international forums. Task Force on AI and Human Rights monitors implementation.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Coalition consensus among member countries. Task Force on Artificial Intelligence and Human Rights coordinates activities. Chairs rotate among member states.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Multi-stakeholder approach including governments, civil society, private sector. Public statements invite non-member states and stakeholders to join commitments.</td></tr>
    <tr><td>Accountability</td><td>Peer accountability among coalition members. Public commitments create reputational stakes. Coordination of advocacy across international forums.</td></tr>
    <tr><td>Remedy Process</td><td>Coalition advocacy for human rights remedies in AI governance. Support for affected populations through diplomatic channels.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Human-centric AI approach. Democratic oversight of AI systems. Protection against fully automated consequential decisions.</td></tr>
    <tr><td>Transparency</td><td>Transparency emphasized as enabler of trust in democratic processes. Disclosure of AI use in government applications.</td></tr>
    <tr><td>Non-Discrimination</td><td>Direct focus on preventing AI from reinforcing inequalities and discrimination. Attention to gender-based violence amplification. Protection of marginalized populations.</td></tr>
    <tr><td>Data Governance</td><td>Privacy rights protection. Opposition to unlawful surveillance enabled by AI. Data protection in AI systems.</td></tr>
    <tr><td>Risk Assessment</td><td>Risk identification in government AI deployment. Attention to high-risk applications in law enforcement, judicial systems, and service delivery.</td></tr>
  </table>

  <h3>Key Statements</h3>
  <div class="principle-box">
    <strong>2025 Joint Statement on Artificial Intelligence and Human Rights</strong>
    <ul style="margin-bottom: 4px;">
      <li>AI holds transformative and disruptive potential</li>
      <li>When governed responsibly, AI can enhance efficiency, transparency, participation, and trust in democratic processes</li>
      <li>Since 2020, rapid AI developments have increased scale and urgency of human rights risks</li>
    </ul>
    <p style="margin-bottom: 4px;"><strong>Identified Harms:</strong></p>
    <ul style="margin-bottom: 4px;">
      <li>AI used to suppress dissent</li>
      <li>Manipulation of public discourse</li>
      <li>Amplification of gender-based violence</li>
      <li>Unlawful and arbitrary digital surveillance</li>
      <li>Reinforcement of inequalities and discrimination</li>
    </ul>
    <p style="margin-bottom: 0;"><strong>Commitment:</strong> Rights-respecting, human-centric, safe, secure, trustworthy and ethical AI future grounded in human rights, accountability, and rule of law.</p>
  </div>
  <div class="principle-box">
    <strong>2024 Joint Statement on Responsible Government Practices for AI Technologies</strong>
    <ul style="margin-bottom: 0;">
      <li>Safe, secure, and trustworthy AI systems offer opportunities for improved public service delivery</li>
      <li>Risks particularly acute in public sector uses &mdash; surveillance, judicial decisions, law enforcement, service delivery</li>
      <li>Biased AI tools can exacerbate existing inequalities and create new forms of marginalization</li>
    </ul>
  </div>

  <h3>Task Forces</h3>
  <table>
    <tr><th>Task Force</th><th>Status</th><th>Focus</th></tr>
    <tr><td>Task Force on Artificial Intelligence and Human Rights (TFAIR)</td><td>Active since 2020</td><td>Coordination of AI advocacy, development of joint positions, engagement with international forums</td></tr>
    <tr><td>Responsible AI in Government Task Force</td><td>Established May 2025</td><td>Drafting best-practice guidance for bias audits, transparency, and human-rights safeguards in high-risk government deployments such as law enforcement and healthcare</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Coalition of 42 democratic nations providing unified voice on AI and human rights. Bridges internet freedom and AI governance. Identifies specific human rights harms from AI (suppression of dissent, surveillance, discrimination). Focus on government AI use and accountability. Coordination mechanism for human rights advocacy in AI forums. Explicitly addresses AI-enabled authoritarianism and surveillance.
  </div>

  <h3>CC Relevance</h3>
  <div class="highlight-box" style="padding: 12px 16px;">
    <strong>Applicability: Medium</strong> &mdash; Provides human rights lens for institutional AI governance
  </div>
  <ul class="relevant-elements">
    <li>Human rights framework applicable to student rights in AI context</li>
    <li>Non-discrimination emphasis directly relevant to CC equity mission</li>
    <li>Transparency requirements for AI affecting students</li>
    <li>Government AI guidance applicable to public institution context</li>
    <li>Protection against AI-enabled discrimination</li>
    <li>Democratic participation in AI governance decisions</li>
    <li>Bias audit best practices (from 2025 Task Force) applicable to EdTech evaluation</li>
    <li>Public service delivery AI guidance relevant to student services</li>
  </ul>
  <p><em>Adaptation Needed:</em> Statement focused on government and democracy context; CC adaptation requires: (1) Translation to educational rights context, (2) Application to vendor AI evaluation, (3) Student voice in AI governance as democratic participation, (4) Bias audit methodologies for educational AI, (5) Connection to existing student rights frameworks. Human rights framing provides powerful foundation for CC AI governance.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 15: IEAIED ETHICAL FRAMEWORK -->
<!-- ============================================================ -->
<div class="framework-card" id="ieaied">
  <div class="framework-header">15. The Ethical Framework for AI in Education</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Institute for Ethical AI in Education (University of Buckingham)</span>
    <span><strong>Year:</strong> 2021</span>
    <span class="meta-tag">Framework</span>
    <span class="meta-tag">International</span>
    <span><strong>Geographic Focus:</strong> Global (UK origin)</span>
    <span><strong>Document Link</strong><a href="https://www.buckingham.ac.uk/wp-content/uploads/2021/03/The-Institute-for-Ethical-AI-in-Education-The-Ethical-Framework-for-AI-in-Education.pdf"> Download PDF</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide a systematic ethical framework for evaluating AI in education, with particular emphasis on procurement decisions. Developed through consultation with 200+ international experts to be applicable across any educational setting.</p>
  <p><strong>Key Principles:</strong> Nine requirements for ethical AI in education: (1) Achieving Educational Goals, (2) Forms of Assessment, (3) Administration &amp; Workload, (4) Equity, (5) Learner Autonomy, (6) Privacy, (7) Transparency &amp; Accountability, (8) Informed Participation, (9) Ethical Design. Includes a procurement checklist for evaluating AI vendors.</p>
  <p><strong>Target Audience:</strong> Education leaders and practitioners making procurement and application decisions; AI suppliers.</p>
  <p><strong>Enforcement:</strong> Voluntary guidelines. Licensed CC BY-NC 4.0 for institutional adaptation.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Administrator-led procurement and application decisions guided by nine-requirement checklist. Framework designed for institutional decision-makers evaluating AI tools.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Developed through extensive global consultation (200+ experts). Emphasizes informed participation of all stakeholders in AI deployment decisions.</td></tr>
    <tr><td>Accountability</td><td>Humans accountable for AI decisions (Requirement 7). Supplier accountability through procurement checklist. No formal enforcement mechanism.</td></tr>
    <tr><td>Remedy Process</td><td>Not formally specified. Transparency and accountability requirements create basis for institutional remedy processes.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Requirement 7 (Transparency &amp; Accountability): Humans must be accountable for AI-influenced decisions. AI should support rather than replace human judgment in educational contexts.</td></tr>
    <tr><td>Transparency</td><td>AI suppliers must demonstrate how their tools meet each of the nine requirements. Stakeholders must understand and consent to AI use (Requirement 8: Informed Participation).</td></tr>
    <tr><td>Non-Discrimination</td><td>Requirement 4 (Equity): AI must promote equity between learner groups. Assessment of whether AI tools serve all learners equitably is part of procurement evaluation.</td></tr>
    <tr><td>Data Governance</td><td>Requirement 6 (Privacy): Balance privacy protection with legitimate educational data use. Procurement checklist evaluates vendor data practices.</td></tr>
    <tr><td>Risk Assessment</td><td>Nine-requirement checklist provides structured evaluation framework. Each requirement has associated questions for vendor assessment during procurement.</td></tr>
  </table>

  <h3>Nine Requirements</h3>
  <div class="principle-box">
    <strong>1. Achieving Educational Goals</strong> &mdash; AI used for well-defined goals with strong evidence of effectiveness.
  </div>
  <div class="principle-box">
    <strong>2. Forms of Assessment</strong> &mdash; Recognize broader aptitudes including metacognition, not just narrow test performance.
  </div>
  <div class="principle-box">
    <strong>3. Administration &amp; Workload</strong> &mdash; Boost institutional capacity while respecting human relationships.
  </div>
  <div class="principle-box">
    <strong>4. Equity</strong> &mdash; Promote equity between learner groups.
  </div>
  <div class="principle-box">
    <strong>5. Learner Autonomy</strong> &mdash; Increase learner control over their own development.
  </div>
  <div class="principle-box">
    <strong>6. Privacy</strong> &mdash; Balance privacy with legitimate data use.
  </div>
  <div class="principle-box">
    <strong>7. Transparency &amp; Accountability</strong> &mdash; Humans accountable for AI decisions.
  </div>
  <div class="principle-box">
    <strong>8. Informed Participation</strong> &mdash; Stakeholders understand and consent to AI use.
  </div>
  <div class="principle-box">
    <strong>9. Ethical Design</strong> &mdash; Resources designed with learners' interests at heart.
  </div>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Only procurement-focused framework in the combined inventory. The nine requirements provide foundational governance architecture that Khan Academy and others have built upon. Developed through extensive international consultation (200+ experts) and designed for &ldquo;any educational setting.&rdquo; The vendor evaluation checklist (pages 4&ndash;9) is immediately actionable for technology selection committees. Licensed CC BY-NC 4.0 for institutional adaptation.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Only procurement-focused framework; directly usable for CC vendor evaluation
  </div>
  <ul class="relevant-elements">
    <li>Procurement checklist directly adaptable to CC technology evaluation and RFPs</li>
    <li>Nine-requirement structure provides systematic governance foundation</li>
    <li>Sector-agnostic design &mdash; explicitly for &ldquo;any educational setting&rdquo;</li>
    <li>Checklist format (pages 4&ndash;9) immediately actionable</li>
    <li>Supplier accountability emphasis &mdash; pushes responsibility to vendors</li>
    <li>Khan Academy adoption validates credibility and ongoing relevance</li>
    <li>Addresses metacognition and broader competency recognition</li>
    <li>CC BY-NC 4.0 license allows institutional adaptation</li>
    <li>Developed through extensive global consultation (200+ experts)</li>
  </ul>
  <p><em>Adaptation Needed:</em> Pre-GenAI (March 2021) &mdash; doesn't address ChatGPT-era concerns such as academic integrity or AI detection. Equity framing less robust than justice-centered frameworks (e.g., Kapor). UK-based policy references may need localization. Institute defunct since 2021, so no ongoing updates. Should be combined with more recent frameworks (TeachAI Toolkit, Alabama Template) for comprehensive coverage. The procurement checklist can be adapted directly into CC vendor evaluation rubrics.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 16: CHAI -->
<!-- ============================================================ -->
<div class="framework-card relevance-high" id="chai">
  <div class="framework-header">16. Coalition for Health AI (CHAI) Responsible Health AI Framework and Assurance Standards</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Coalition for Health AI (CHAI)</span>
    <span><strong>Year:</strong> 2024&ndash;2025</span>
    <span class="meta-tag">Standard</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States / Global Healthcare</span>
    <span><strong>Link:</strong> <a href="https://www.chai.org/">chai.org</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Advance the responsible development, deployment, and oversight of AI in healthcare through multi-stakeholder consensus, shared assurance standards, and independent certification infrastructure.</p>
  <p><strong>Key Principles:</strong> Five core principles: (1) Usability and Efficacy, (2) Safety and Reliability, (3) Transparency, (4) Equity, (5) Data Security and Privacy. Aligned with NAM AI Code of Conduct, White House Blueprint for an AI Bill of Rights, and NIST frameworks.</p>
  <p><strong>Target Audience:</strong> Healthcare systems, hospitals, technology developers, government agencies, patient advocacy groups, and quality assurance laboratories.</p>
  <p><strong>Enforcement:</strong> Voluntary consensus-based standards with movement toward certification. Joint Commission partnership will develop voluntary AI certification for 22,000+ accredited healthcare organizations. ANSI/ISO 17025 lab certification framework.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Consensus-based multi-stakeholder development. Board of Directors (chaired by Dr. John Halamka, Mayo Clinic Platform) with CEO (Dr. Brian Anderson, MITRE). Work groups develop standards through open collaborative processes with public comment periods.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Open membership: 3,000+ member organizations including academic medical centers, regional/rural health systems, tech companies (Microsoft, Google, Amazon), government agencies, patient advocates, and startups. Tiered membership by size/type ensures accessibility.</td></tr>
    <tr><td>Accountability</td><td>Independent Assurance Labs certified under ANSI/ISO 17025. CHAI Model Cards (&ldquo;nutrition labels&rdquo;) standardize reporting. Joint Commission partnership extends governance to 80%+ of US healthcare organizations.</td></tr>
    <tr><td>Remedy Process</td><td>Voluntary, blinded reporting of AI safety events to independent patient safety organizations. Formal AI governance structures with designated experienced individuals at each institution.</td></tr>
  </table>

  <h3>Key Provisions</h3>
  <table>
    <tr><th style="width:25%">Provision</th><th>Details</th></tr>
    <tr><td>Human Oversight</td><td>Governance structures require designated individuals with appropriate experience. Independent expert review at each lifecycle stage.</td></tr>
    <tr><td>Transparency</td><td>Model Cards as standardized &ldquo;nutrition labels&rdquo; for AI systems. Patient-facing disclosure mechanisms. Public comment periods for framework development.</td></tr>
    <tr><td>Non-Discrimination</td><td>Equity as core principle. Real-world use cases address diverse populations. Tiered membership ensures all organization sizes participate in governance.</td></tr>
    <tr><td>Data Governance</td><td>Incorporates NIST Privacy Framework and Cybersecurity Framework. Assurance Reporting Checklists evaluate data standards across AI lifecycle.</td></tr>
    <tr><td>Risk Assessment</td><td>Lifecycle framework: problem definition/planning, ethical design/engineering, deployment/monitoring. Assurance Reporting Checklists across complete AI lifecycle.</td></tr>
  </table>

  <h3>Network Governance Model</h3>
  <div class="callout">
    <strong>Why this matters for CC networks:</strong> CHAI demonstrates how a sector can move from principles to operational standards through: (1) consensus-based standard development with 3,000+ members, (2) independent assurance lab certification (ISO 17025), (3) standardized reporting via Model Cards, (4) partnership with existing accreditation body (Joint Commission) to scale adoption, (5) tiered implementation playbooks recognizing resource disparities across organization sizes.
  </div>
  <p><strong>Consortium Structure:</strong> Grew from small clinician group (2021) to 3,000+ member organizations. Open membership with tiered dues by nonprofit/for-profit status and organization size.</p>
  <p><strong>Shared Standards:</strong> Work groups develop standards through consensus with 60-day public comment periods. Framework aligns with multiple federal standards for interoperability.</p>
  <p><strong>Independent Evaluation:</strong> Assurance Labs certified under ANSI/ISO 17025 provide third-party testing. Model Card registry standardizes reporting across institutions.</p>
  <p><strong>Local Adaptation:</strong> Joint Commission governance playbooks will be tailored to different organization sizes. Working groups segregated by organization type and size. Institutions implement within CHAI&rsquo;s consensus framework while adapting to local context.</p>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Most advanced model of network-based AI governance with shared evaluation infrastructure. Shows how institutions can collectively negotiate AI governance standards while maintaining institutional autonomy in implementation. The Joint Commission partnership demonstrates how to leverage existing accreditation bodies for AI governance scale.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Structural consortium model transferable to CC networks
  </div>
  <ul class="relevant-elements">
    <li>Consortium governance model transferable to CC networks: consensus development, open membership, tiered participation</li>
    <li>Assurance lab model suggests shared evaluation infrastructure for CC AI vendor assessment</li>
    <li>Model Cards / &ldquo;nutrition labels&rdquo; concept applicable to CC AI tool transparency</li>
    <li>Tiered playbooks by organization size addresses CC resource constraints</li>
    <li>Partnership with existing accreditation body mirrors potential SACSCOC/ACCJC collaboration</li>
    <li>Working groups segregated by organization type applicable to CC distinctions</li>
    <li>Public comment periods demonstrate inclusive governance process</li>
    <li>Voluntary certification provides path between mandates and aspiration</li>
  </ul>
  <p><em>Adaptation Needed:</em> Healthcare context requires translation to education: patient safety &rarr; student outcomes, clinical AI &rarr; educational AI, HIPAA &rarr; FERPA, Joint Commission &rarr; SACSCOC/ACCJC. But structural governance model (multi-stakeholder consortium, shared standards, independent evaluation, tiered implementation) is directly transferable.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 17: WCET -->
<!-- ============================================================ -->
<div class="framework-card relevance-high" id="wcet">
  <div class="framework-header">17. WCET AI Education Policy &amp; Practice Ecosystem Framework</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> WICHE Cooperative for Educational Technologies (WCET)</span>
    <span><strong>Year:</strong> 2023&ndash;2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States / Higher Education</span>
    <span><strong>Link:</strong> <a href="https://wcet.wiche.edu/resources/ai-education-policy-practice-ecosystem-framework/">WCET AI Framework</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide a strategic roadmap for institutions to build adaptable AI policies through a three-tiered ecosystem approach addressing governance, operations, and pedagogy. Help institutions filter competing demands and allocate resources effectively.</p>
  <p><strong>Key Principles:</strong> Three-dimensional approach: (1) Governance, (2) Operations, (3) Pedagogy (Instruction and Learning). Adapted from Cecilia Ka Yuk Chan&rsquo;s AI Ecological Education Policy Framework.</p>
  <p><strong>Target Audience:</strong> Higher education institutions, administrators, faculty, and staff. Cooperative serving colleges, universities, and organizations focused on digital learning.</p>
  <p><strong>Enforcement:</strong> Voluntary, member-driven. Cooperative model where members share solutions and lessons learned. Member-only toolkit provides practical resources.</p>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Cooperative membership model. WCET is a member-driven nonprofit under WICHE (Western Interstate Commission for Higher Education), established 1988. AI Working Group develops and maintains framework.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Open to colleges, universities, higher ed organizations, and companies. 224 survey respondents and 7 professional interviews inform 2025 iteration. Organization-wide membership provides access to all employees and students.</td></tr>
    <tr><td>Accountability</td><td>Annual surveys benchmark institutional AI maturity. Prioritized implementation roadmap. Self-assessment against three dimensions.</td></tr>
    <tr><td>Remedy Process</td><td>Iterative framework revision based on member input. 2025 revision moved beyond initial checklist to &ldquo;action-oriented, strategic tool&rdquo; based on feedback.</td></tr>
  </table>

  <h3>Framework Dimensions</h3>
  <div class="area-box">
    <strong>Governance</strong> &mdash; Institutional policies, decision-making structures, and oversight mechanisms for AI adoption and use.
  </div>
  <div class="area-box">
    <strong>Operations</strong> &mdash; Administrative and operational AI applications, IT infrastructure, procurement, and resource allocation.
  </div>
  <div class="area-box">
    <strong>Pedagogy</strong> &mdash; Instruction and learning applications, assessment design, academic integrity, and student-facing AI use.
  </div>

  <h3>Framework Evolution</h3>
  <table>
    <tr><th style="width:25%">Version</th><th>Description</th></tr>
    <tr><td>v1 (Dec 2023)</td><td>Initial guide identifying key areas of concern. Adapted from Chan&rsquo;s AI Ecological Education Policy Framework.</td></tr>
    <tr><td>Toolkit (Dec 2023)</td><td>Member-only practical application toolkit for real-world policy alignment.</td></tr>
    <tr><td>v2 (Oct 2025)</td><td>Prioritized implementation roadmap with policy examples. More complex, action-oriented, and strategic.</td></tr>
  </table>

  <h3>Network Governance Model</h3>
  <div class="callout">
    <strong>Why this matters for CC networks:</strong> WCET is an operating cooperative since 1988 that demonstrates how member institutions can pool resources, share learnings, and collectively develop AI governance frameworks. The cooperative exchange model&mdash;shared surveys, working groups, member-only toolkits, iterative revision&mdash;is directly applicable to a CC network approach.
  </div>
  <p><strong>Survey Findings (2025):</strong> 224 institutional responses. Most institutions &ldquo;slightly mature&rdquo; in AI use. Top barrier: lack of knowledge among administrators, staff, and faculty.</p>
  <p><strong>&ldquo;Framework Fog&rdquo;:</strong> AI Working Group identified overwhelming number of AI frameworks creating paralysis of choice&mdash;validates need for curated, CC-specific guidance.</p>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Operating cooperative model for higher education AI governance. Demonstrates how a member-driven organization can develop shared frameworks through working groups, create benchmarking through annual surveys, provide member-only toolkits, and iterate based on member experience. WICHE context is particularly relevant to Western states including California.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Cooperative model directly applicable to CC networks
  </div>
  <ul class="relevant-elements">
    <li>Cooperative membership model provides template for CC network governance</li>
    <li>Three-dimensional framework (governance, operations, pedagogy) maps to CC structure</li>
    <li>Organization-wide membership ensures adjunct faculty access to resources</li>
    <li>Annual survey benchmarking applicable to CC self-assessment</li>
    <li>&ldquo;Framework fog&rdquo; concept validates need for curated, CC-specific guidance</li>
    <li>Iterative revision (v1 2023 to v2 2025) shows sustainable development model</li>
    <li>Digital learning focus connects to online/hybrid CC programs and Calbright</li>
    <li>WICHE context relevant to Western states including California</li>
  </ul>
  <p><em>Adaptation Needed:</em> WCET focuses on digital learning rather than AI governance broadly. CC adaptation: expand beyond digital learning, address CTE/workforce specifically, incorporate equity as central dimension, include vendor procurement governance. Three dimensions are a starting point but CC seven-pillar model is more comprehensive.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 18: NACUBO -->
<!-- ============================================================ -->
<div class="framework-card" id="nacubo">
  <div class="framework-header" style="background: #856404;">18. NACUBO AI Governance &amp; Cooperative Procurement Ecosystem</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> NACUBO / E&amp;I Cooperative Services</span>
    <span><strong>Year:</strong> 2024&ndash;2025</span>
    <span class="meta-tag">Platform</span>
    <span class="meta-tag">Sector</span>
    <span><strong>Geographic Focus:</strong> United States / Higher Education</span>
    <span><strong>Link:</strong> <a href="https://www.nacubo.org/">nacubo.org</a> | <a href="https://www.eandi.org/">eandi.org</a></span>
  </div>

  <div class="highlight-box">
    <strong>Note:</strong> NACUBO does not have a standalone AI governance framework. This profile documents NACUBO&rsquo;s AI governance <em>ecosystem</em>&mdash;collaborative research, conference programming, and connection to cooperative procurement infrastructure.
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Convening and resource-sharing platform for higher education business officers navigating AI adoption. Primary AI-relevant contributions: (1) collaborative research with EDUCAUSE/CUPA-HR/AIR on AI workforce impact, (2) conference programming on AI readiness, (3) connection to E&amp;I Cooperative Services for collective procurement.</p>
  <p><strong>Target Audience:</strong> College and university business officers, CFOs, procurement professionals, and administrative leadership.</p>

  <h3>Collaborative Research</h3>
  <table>
    <tr><th style="width:25%">Finding</th><th>Details</th></tr>
    <tr><td>Report</td><td><em>The Impact of AI on Work in Higher Education</em> &mdash; Joint research with EDUCAUSE, CUPA-HR, AIR (2,000+ staff surveyed)</td></tr>
    <tr><td>AI Usage</td><td>70%+ of surveyed professionals use AI tools daily or weekly</td></tr>
    <tr><td>Shadow AI</td><td><strong>56% use AI tools not provided by their institutions</strong> &mdash; data privacy/security implications</td></tr>
    <tr><td>Strategy</td><td>92% of institutions have some form of AI strategy (piloting, evaluating, encouraging use)</td></tr>
    <tr><td>Risk Concerns</td><td>Over half express concerns about risks accompanying AI&rsquo;s potential</td></tr>
  </table>

  <h3>Cooperative Procurement Ecosystem</h3>
  <div class="callout">
    <strong>E&amp;I Cooperative Services</strong> &mdash; The only member-owned, nonprofit sourcing cooperative dedicated exclusively to education. Founded 1934. 6,400+ member institutions. 215+ competitively solicited, compliant contracts.
  </div>
  <p><strong>AI-Relevant Contracts:</strong></p>
  <ul class="relevant-elements">
    <li><strong>New Tech Solutions:</strong> Generative AI platform deployment, secure governance frameworks, automation solutions, implementation services and training</li>
    <li><strong>Slalom Consulting:</strong> AI &amp; Data Strategy (admissions, enrollment, student data, advancement)</li>
    <li><strong>Aleysian:</strong> CRM and AI-driven solutions (Salesforce, Einstein Analytics) for recruitment, retention, alumni</li>
    <li><strong>Transcepta:</strong> AI-powered accounts payable automation (2025&ndash;2030 contract)</li>
  </ul>
  <p><strong>Shared Services Precedent:</strong> Cal State University identified 15 shared service opportunities starting with procurement, cybersecurity, and benefits management. Procurement initiative alone: one-time savings of $20.7 million and $3M annual savings from getting every campus onto the same digital sourcing platform.</p>

  <h3>Network Governance Model</h3>
  <div class="callout">
    <strong>Why this matters for CC networks:</strong> NACUBO&rsquo;s value is showing how professional associations and cooperative procurement structures create network infrastructure for AI governance. E&amp;I Cooperative Services (6,400+ members, 215+ contracts) is the most concrete example of collective purchasing power in higher education. The Cal State shared services model ($20.7M savings) demonstrates multi-campus procurement consolidation in California.
  </div>

  <h3>CC Relevance</h3>
  <div class="success" style="padding: 12px 16px; background: #fff3cd; border-left: 4px solid #ffc107;">
    <strong>Applicability: Medium-High</strong> &mdash; Procurement model high relevance; governance framework low relevance
  </div>
  <ul class="relevant-elements">
    <li>E&amp;I Cooperative model directly applicable to CC collective AI procurement</li>
    <li>GenAI-specific cooperative contract (New Tech Solutions) already exists</li>
    <li>Cal State shared services example highly relevant to California CC system</li>
    <li>Shadow AI data (56%) supports case for institutional AI governance</li>
    <li>Cross-association research model (EDUCAUSE/CUPA-HR/AIR) applicable to CC professional organizations</li>
    <li>Community Exchange resource sharing for CC networks</li>
  </ul>
  <p><em>Adaptation Needed:</em> E&amp;I contracts focused on administrative rather than instructional AI. No CTE-specific procurement guidance. CC-specific shared services model needed (not just university model). Need to connect procurement governance to educational governance pillars.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 19: GIDA -->
<!-- ============================================================ -->
<div class="framework-card relevance-high" id="gida">
  <div class="framework-header">19. GIDA: CARE Principles &amp; Indigenous Peoples&rsquo; Rights in Data</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Global Indigenous Data Alliance (GIDA)</span>
    <span><strong>Year:</strong> 2019&ndash;2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">International</span>
    <span><strong>Geographic Focus:</strong> Global (federated national bodies)</span>
    <span><strong>Link:</strong> <a href="https://www.gida-global.org/">gida-global.org</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Advance Indigenous Peoples&rsquo; control over Indigenous data through a federated international alliance. Establish minimum expectations for guiding the inclusion of Indigenous Peoples in data governance. Move beyond individual consent to collective data sovereignty.</p>
  <p><strong>Key Principles:</strong> CARE Principles for Indigenous Data Governance: <strong>C</strong>ollective benefit, <strong>A</strong>uthority to control, <strong>R</strong>esponsibility, <strong>E</strong>thics. Complemented by 12 Indigenous Peoples&rsquo; Rights in Data.</p>
  <p><strong>Enforcement:</strong> Principles-based with growing policy adoption. Incorporated into AIATSIS Code of Ethics, draft UNESCO Recommendation on Open Science, and IEEE Recommended Practice for Provenance of Indigenous Peoples&rsquo; Data.</p>

  <h3>CARE Principles</h3>
  <div class="principle-box">
    <strong>C &mdash; Collective Benefit:</strong> Data ecosystems shall enable Indigenous Peoples to derive benefit from the data.
  </div>
  <div class="principle-box">
    <strong>A &mdash; Authority to Control:</strong> Indigenous Peoples&rsquo; rights and interests in Indigenous data must be recognized and their authority to control such data must be empowered.
  </div>
  <div class="principle-box">
    <strong>R &mdash; Responsibility:</strong> Those working with Indigenous data have a responsibility to share how data are used to support self-determination and collective benefit.
  </div>
  <div class="principle-box">
    <strong>E &mdash; Ethics:</strong> Indigenous Peoples&rsquo; rights and wellbeing should be the primary concern at all stages of the data life cycle.
  </div>

  <h3>Federated Network Structure</h3>
  <table>
    <tr><th style="width:25%">National Member</th><th>Jurisdiction</th></tr>
    <tr><td>Te Mana Raraunga</td><td>Aotearoa / New Zealand &mdash; M&#257;ori data sovereignty</td></tr>
    <tr><td>Maiam nayri Wingara</td><td>Australia &mdash; Aboriginal and Torres Strait Islander data sovereignty</td></tr>
    <tr><td>FNIGC</td><td>Canada &mdash; First Nations data governance (developed OCAP&reg; Principles)</td></tr>
    <tr><td>USIDSN</td><td>United States &mdash; US tribal data sovereignty</td></tr>
    <tr><td>GIDA-S&aacute;pmi</td><td>Nordic region &mdash; S&aacute;mi peoples&rsquo; data governance</td></tr>
  </table>
  <p><strong>Operating Principle:</strong> National organizations maintain full autonomy over their governance and priorities. GIDA provides international coordination, advocacy, and knowledge sharing without imposing uniform standards.</p>

  <h3>Network Governance Model</h3>
  <div class="callout">
    <strong>Why this matters for CC networks:</strong> GIDA is the most developed example of federated governance in our research inventory. National organizations maintain complete autonomy while sharing principles, advocacy platforms, and knowledge. This &ldquo;network of networks&rdquo; demonstrates how collective governance works without homogenizing local approaches&mdash;directly applicable to how a CC network could operate with shared pillars but institutional independence.
  </div>
  <p><strong>Shared Principles, Local Implementation:</strong> CARE Principles provide common minimum expectations globally. Each national organization adapts to local legal, cultural, and political contexts.</p>
  <p><strong>Collective Development:</strong> The 12 Rights were developed through inductive collaborative working group process, reviewing literature and implementation activities across nations.</p>
  <p><strong>Conference Governance:</strong> GIDSov 2025 (Canberra, 250+ participants from 12+ countries) serves as collective accountability and direction-setting venue. Next conference 2027, Canada.</p>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Most developed federated governance model in the inventory. Demonstrates: (1) shared principles (CARE) without uniform standards, (2) national/local autonomy with collective voice, (3) international policy influence from federated position, (4) collaborative cross-national working groups, (5) conference-based governance and accountability. The collective consent model (vs. individual consent) is directly relevant to how CC networks might approach shared data governance.
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Governance model, not Indigenous-specific content
  </div>
  <ul class="relevant-elements">
    <li>Federated &ldquo;network of networks&rdquo; structure applicable to CC system governance (state + individual colleges)</li>
    <li>CARE Principles model (shared values, local implementation) maps to CC framework adoption</li>
    <li>Collective benefit principle aligns with CC community mission</li>
    <li>National organizations maintaining autonomy mirrors CC institutional independence</li>
    <li>Collaborative working group process for shared standards is replicable</li>
    <li>Conference-as-governance mechanism applicable to CC network convenings</li>
    <li>Challenges individual-only consent &mdash; relevant to institutional data pooling</li>
  </ul>
  <p><em>Adaptation Needed:</em> GIDA&rsquo;s context is fundamentally different (colonial data relationships, Indigenous sovereignty). Structural lessons are transferable but must not appropriate Indigenous frameworks. CC adaptation should acknowledge GIDA as inspiration while building a sector-specific model, adapting the federated structure for multi-institution governance while respecting institutional autonomy.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 20: DATA COOPERATIVES -->
<!-- ============================================================ -->
<div class="framework-card relevance-high" id="data-coops">
  <div class="framework-header">20. Data Cooperatives Governance Model: A Practitioner&rsquo;s Handbook</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> Aapti Institute / Data2X / Open Data Manchester</span>
    <span><strong>Year:</strong> 2024</span>
    <span class="meta-tag">Toolkit</span>
    <span class="meta-tag">International</span>
    <span><strong>Geographic Focus:</strong> Global</span>
    <span><strong>Link:</strong> <a href="https://data2x.org/resource-center/how-to-build-a-data-cooperative-a-practitioners-handbook/">Data2X Handbook</a></span>
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Practical guide for forming data cooperatives&mdash;member-owned organizations that pool data for mutual benefit. Addresses data governance as collective rather than individual challenge. Supports communities (especially marginalized populations) in controlling data about their own lives.</p>
  <p><strong>Key Principles:</strong> Based on International Cooperative Alliance (ICA) model: autonomous association of persons united voluntarily to meet common needs. Five characteristics: cooperativism, data subject-centeredness, subversiveness, collective action, fiduciary responsibility.</p>
  <p><strong>Enforcement:</strong> Voluntary, member-governed. Aligned with EU Data Governance Act provisions for data intermediaries.</p>

  <h3>Four-Step Process</h3>
  <div class="principle-box">
    <strong>Step 1: Determine Fit</strong> &mdash; Assess whether cooperative model fits the community&rsquo;s needs, capacity, and goals.
  </div>
  <div class="principle-box">
    <strong>Step 2: Value Proposition</strong> &mdash; Name the cooperative&rsquo;s purpose and fundamental values. Clarify mutual benefit from pooling data.
  </div>
  <div class="principle-box">
    <strong>Step 3: Governance Architecture</strong> &mdash; Establish agreed-upon governance through participatory process. The cooperative succeeds or fails based on the strength of members&rsquo; relationships. Well-constructed frameworks build and reinforce trust.
  </div>
  <div class="principle-box">
    <strong>Step 4: Data Infrastructure</strong> &mdash; Build technical systems for pooling, sharing, and analysis. The most useful insights emerge when individual data is pooled, revealing relationships individual datasets cannot.
  </div>

  <h3>Governance Model</h3>
  <table>
    <tr><th style="width:25%">Aspect</th><th>Description</th></tr>
    <tr><td>Decision Structure</td><td>Single-stakeholder model with equal voting rights. Democratic governance following cooperative principles (one member, one vote). Participatory co-design process.</td></tr>
    <tr><td>Stakeholder Participation</td><td>Data subjects are members with equal voting rights. Open membership for those who share data.</td></tr>
    <tr><td>Accountability</td><td>Member-governed through cooperative structures. Consent systems with granular control. Fiduciary responsibility to members.</td></tr>
    <tr><td>Remedy Process</td><td>Members can withdraw consent and data at any time. Democratic remedy through voting and governance participation. Fiduciary duty creates legal accountability.</td></tr>
  </table>

  <h3>Network Governance Model</h3>
  <div class="callout">
    <strong>Why this matters for CC networks:</strong> Most practical toolkit for building collective data governance from scratch. Key insight: data governance can be organized cooperatively with member institutions pooling data for mutual benefit while maintaining individual control. The fiduciary responsibility model and trust-building emphasis are directly applicable to multi-institution data sharing agreements.
  </div>
  <p><strong>Balance of Collective and Individual:</strong> Members maintain autonomy over their data (can withdraw at any time) while participating in collective governance and benefiting from pooled data.</p>
  <p><strong>Trust-Building:</strong> Governance architecture explicitly designed to build and reinforce trust. Handbook emphasizes success depends on strength of member relationships.</p>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Most actionable toolkit for establishing collective data governance. While other frameworks describe principles, this handbook provides step-by-step process. Equal voting rights protect smaller members. Participatory co-design methodology is transferable. Oriented toward marginalized populations&mdash;&ldquo;controlling data about our own lives allows us to better represent ourselves, especially relevant for marginalized people who may be invisible to social benefits.&rdquo;
  </div>

  <h3>CC Relevance</h3>
  <div class="success relevance-high" style="padding: 12px 16px;">
    <strong>Applicability: High</strong> &mdash; Governance model directly applicable to CC data pooling
  </div>
  <ul class="relevant-elements">
    <li>Four-step process directly applicable to establishing CC data governance cooperative</li>
    <li>Equal voting rights model protects smaller CCs in network governance</li>
    <li>Fiduciary responsibility applicable to system-level data governance</li>
    <li>Participatory co-design methodology for governance architecture</li>
    <li>Balance of collective benefit and individual autonomy maps to CC independence</li>
    <li>Trust-building emphasis critical for multi-institution collaboration</li>
    <li>Marginalized population focus aligns with CC equity mission</li>
    <li>Handbook format (practical, step-by-step) matches CC resource constraints</li>
  </ul>
  <p><em>Adaptation Needed:</em> Model assumes individual data subjects as members. CC adaptation: institutions as members rather than individuals, FERPA and institutional obligations rather than individual consent alone, vendor procurement dimension not addressed, AI governance beyond data governance needs additional pillars, Perkins V and accreditation reporting not addressed.</p>
</div>

<!-- ============================================================ -->
<!-- FRAMEWORK 21: HEAT-AI -->
<!-- ============================================================ -->
<div class="framework-card" id="heat-ai">
  <div class="framework-header" style="background: #6c757d;">21. HEAT-AI: Higher Education Act for AI</div>

  <div class="framework-meta">
    <span><strong>Organization:</strong> St. P&ouml;lten University of Applied Sciences (Austria)</span>
    <span><strong>Year:</strong> 2024&ndash;2025</span>
    <span class="meta-tag">Guidance</span>
    <span class="meta-tag">Institutional</span>
    <span><strong>Geographic Focus:</strong> Austria / EU</span>
    <span><strong>Authors:</strong> Marlies Temper, Simon Tjoa, Lisa David</span>
    <span><strong>Link:</strong> <a href="https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1505370/full">Frontiers in Education</a></span>
  </div>

  <div class="highlight-box">
    <strong>Note:</strong> HEAT-AI is a single-institution framework, <em>not</em> a multi-institutional consortium as originally catalogued. Developed at and for St. P&ouml;lten University of Applied Sciences. Published open-access for broader adoption.
  </div>

  <h3>Summary</h3>
  <p><strong>Purpose:</strong> Provide a structured, risk-based framework for regulating AI usage in higher education, adapted from the EU AI Act. Encourages AI adoption while maintaining academic integrity and ethical use.</p>
  <p><strong>Key Principles:</strong> (1) Students and faculty shall be encouraged to use AI, (2) Academic integrity shall not be impacted, (3) AI shall be used ethically and lawfully without violating privacy.</p>
  <p><strong>Enforcement:</strong> Institutional policy approved by University Board, September 2024. Published as peer-reviewed open-access paper (Frontiers in Education, Feb 2025).</p>

  <h3>Risk-Based Framework (Adapted from EU AI Act)</h3>
  <div class="critical" style="border-left-color: #dc3545;">
    <strong>Unacceptable Risk (Prohibited)</strong> &mdash; AI uses violating legal requirements or compromising academic integrity in ways that cannot be mitigated.
  </div>
  <div class="highlight-box" style="border-color: #dc3545;">
    <strong>High Risk (Extensive safeguards)</strong> &mdash; Automated grading/assessment, evaluation of student academic progress, academic integrity evaluation, teaching effectiveness evaluation, adaptive learning systems, individual course content/learning pathways, monitoring students using academic data.
  </div>
  <div class="callout">
    <strong>Limited Risk (Transparency obligations)</strong> &mdash; Uses where insufficient transparency could create harm. Requires &ldquo;AI generated&rdquo; declarations.
  </div>
  <div class="about-box" style="font-style: normal;">
    <strong>Minimal Risk (Unrestricted)</strong> &mdash; Low-impact AI uses with no significant risk. Permitted without restriction.
  </div>

  <h3>Development Timeline</h3>
  <table>
    <tr><th style="width:25%">Date</th><th>Milestone</th></tr>
    <tr><td>March 2023</td><td>Open space with 23 bachelor/master program directors</td></tr>
    <tr><td>April 2023 &ndash; March 2024</td><td>Round tables for iterative development; comparative study of peer institutions; EU AI Act analysis</td></tr>
    <tr><td>June 2024</td><td>First draft completed</td></tr>
    <tr><td>September 2024</td><td>Approved by University Board, went live</td></tr>
    <tr><td>February 2025</td><td>Peer-reviewed article published (Frontiers in Education, DOI: 10.3389/feduc.2025.1505370)</td></tr>
    <tr><td>Planned</td><td>COBIT-based governance layer (future work); evaluation end of academic year 2025</td></tr>
  </table>

  <h3>Unique Contribution</h3>
  <div class="callout">
    Only framework that directly translates the EU AI Act&rsquo;s risk-based approach into institutional higher education policy. Provides concrete use cases for each risk category. The 12+ month collaborative development process demonstrates a practical governance development methodology. Open access (CC BY) for institutional adaptation.
  </div>

  <h3>CC Relevance</h3>
  <div style="padding: 12px 16px; background: #f0f0f0; border-left: 4px solid #6c757d;">
    <strong>Applicability: Medium</strong> &mdash; Useful process model, requires significant adaptation
  </div>
  <ul class="relevant-elements">
    <li>Risk-based classification system applicable to CC AI governance</li>
    <li>Collaborative development process with program directors transferable to shared governance</li>
    <li>High-risk use case inventory (grading, assessment, learning pathways) relevant to CC</li>
    <li>12-month development timeline is realistic for CC adoption</li>
    <li>Encouragement of AI use (not just restriction) aligns with workforce preparation</li>
  </ul>
  <p><em>Adaptation Needed:</em> EU AI Act context needs US translation. Single-institution without network dimension. No equity/social justice lens. No CTE provisions. Resource constraints not addressed. Adjunct faculty absent. Most valuable as process model (how to develop institutional AI policy) rather than content model.</p>
</div>

<!-- ============================================================ -->
<!-- FOOTER -->
<!-- ============================================================ -->

<div class="about-box">
These framework profiles were extracted as part of the Global AI Governance Research project. Each profile documents a framework's structure, provisions, and relevance to California community colleges.
</div>

<hr>

<p class="metadata" style="text-align: center;">
<em>Prepared by AQL Labs / FutureObjects</em><br>
<em>Commissioned by College Futures Foundation | February 2026</em>
</p>

</body>
</html>
